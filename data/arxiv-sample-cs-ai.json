[
  {
    "id": "0704.0047",
    "submitter": "Igor Grabec",
    "authors": "T. Kosel and I. Grabec",
    "title": "Intelligent location of simultaneously active acoustic emission sources:\n  Part I",
    "comments": "5 pages, 5 eps figures, uses IEEEtran.cls",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI",
    "license": null,
    "abstract": "  The intelligent acoustic emission locator is described in Part I, while Part\nII discusses blind source separation, time delay estimation and location of two\nsimultaneously active continuous acoustic emission sources.\n  The location of acoustic emission on complicated aircraft frame structures is\na difficult problem of non-destructive testing. This article describes an\nintelligent acoustic emission source locator. The intelligent locator comprises\na sensor antenna and a general regression neural network, which solves the\nlocation problem based on learning from examples. Locator performance was\ntested on different test specimens. Tests have shown that the accuracy of\nlocation depends on sound velocity and attenuation in the specimen, the\ndimensions of the tested area, and the properties of stored data. The location\naccuracy achieved by the intelligent locator is comparable to that obtained by\nthe conventional triangulation method, while the applicability of the\nintelligent locator is more general since analysis of sonic ray paths is\navoided. This is a promising method for non-destructive testing of aircraft\nframe structures by the acoustic emission method.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 1 Apr 2007 13:06:50 GMT"
      }
    ],
    "update_date": "2009-09-29",
    "authors_parsed": [
      [
        "Kosel",
        "T.",
        ""
      ],
      [
        "Grabec",
        "I.",
        ""
      ]
    ]
  },
  {
    "id": "0704.0050",
    "submitter": "Igor Grabec",
    "authors": "T. Kosel and I. Grabec",
    "title": "Intelligent location of simultaneously active acoustic emission sources:\n  Part II",
    "comments": "5 pages, 7 eps figures, uses IEEEtran.cls",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI",
    "license": null,
    "abstract": "  Part I describes an intelligent acoustic emission locator, while Part II\ndiscusses blind source separation, time delay estimation and location of two\ncontinuous acoustic emission sources.\n  Acoustic emission (AE) analysis is used for characterization and location of\ndeveloping defects in materials. AE sources often generate a mixture of various\nstatistically independent signals. A difficult problem of AE analysis is\nseparation and characterization of signal components when the signals from\nvarious sources and the mode of mixing are unknown. Recently, blind source\nseparation (BSS) by independent component analysis (ICA) has been used to solve\nthese problems. The purpose of this paper is to demonstrate the applicability\nof ICA to locate two independent simultaneously active acoustic emission\nsources on an aluminum band specimen. The method is promising for\nnon-destructive testing of aircraft frame structures by acoustic emission\nanalysis.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 1 Apr 2007 18:53:13 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Kosel",
        "T.",
        ""
      ],
      [
        "Grabec",
        "I.",
        ""
      ]
    ]
  },
  {
    "id": "0704.0304",
    "submitter": "Carlos Gershenson",
    "authors": "Carlos Gershenson",
    "title": "The World as Evolving Information",
    "comments": "16 pages. Extended version, three more laws of information, two\n  classifications, and discussion added. To be published (soon) in\n  International Conference on Complex Systems 2007 Proceedings",
    "journal-ref": "Minai, A., Braha, D., and Bar-Yam, Y., eds. Unifying Themes in\n  Complex Systems VII, pp. 100-115. Springer, Berlin Heidelberg, 2012",
    "doi": "10.1007/978-3-642-18003-3_10",
    "report-no": null,
    "categories": "cs.IT cs.AI math.IT q-bio.PE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This paper discusses the benefits of describing the world as information,\nespecially in the study of the evolution of life and cognition. Traditional\nstudies encounter problems because it is difficult to describe life and\ncognition in terms of matter and energy, since their laws are valid only at the\nphysical scale. However, if matter and energy, as well as life and cognition,\nare described in terms of information, evolution can be described consistently\nas information becoming more complex.\n  The paper presents eight tentative laws of information, valid at multiple\nscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\npsychological, philosophical, and complexity principles. These are further used\nto discuss the notions of life, cognition and their evolution.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 Apr 2007 02:08:48 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 30 Aug 2007 20:03:59 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 13 Oct 2010 19:49:16 GMT"
      }
    ],
    "update_date": "2013-04-05",
    "authors_parsed": [
      [
        "Gershenson",
        "Carlos",
        ""
      ]
    ]
  },
  {
    "id": "0704.0985",
    "submitter": "Mohd Abubakr",
    "authors": "Mohd Abubakr, R.M.Vinay",
    "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems",
    "comments": "4 pages, 2 figures. Submitted to SASO 2007",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI",
    "license": null,
    "abstract": "  Advances in semiconductor technology are contributing to the increasing\ncomplexity in the design of embedded systems. Architectures with novel\ntechniques such as evolvable nature and autonomous behavior have engrossed lot\nof attention. This paper demonstrates conceptually evolvable embedded systems\ncan be characterized basing on acausal nature. It is noted that in acausal\nsystems, future input needs to be known, here we make a mechanism such that the\nsystem predicts the future inputs and exhibits pseudo acausal nature. An\nembedded system that uses theoretical framework of acausality is proposed. Our\nmethod aims at a novel architecture that features the hardware evolability and\nautonomous behavior alongside pseudo acausality. Various aspects of this\narchitecture are discussed in detail along with the limitations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 7 Apr 2007 13:40:49 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Abubakr",
        "Mohd",
        ""
      ],
      [
        "Vinay",
        "R. M.",
        ""
      ]
    ]
  },
  {
    "id": "0704.1028",
    "submitter": "Jianlin Cheng",
    "authors": "Jianlin Cheng",
    "title": "A neural network approach to ordinal regression",
    "comments": "8 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.NE",
    "license": null,
    "abstract": "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 8 Apr 2007 17:36:00 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Cheng",
        "Jianlin",
        ""
      ]
    ]
  },
  {
    "id": "0704.1394",
    "submitter": "Tarik Had\\v{z}i\\'c",
    "authors": "Tarik Hadzic, Rune Moller Jensen, Henrik Reif Andersen",
    "title": "Calculating Valid Domains for BDD-Based Interactive Configuration",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Apr 2007 10:59:56 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Hadzic",
        "Tarik",
        ""
      ],
      [
        "Jensen",
        "Rune Moller",
        ""
      ],
      [
        "Andersen",
        "Henrik Reif",
        ""
      ]
    ]
  },
  {
    "id": "0704.1409",
    "submitter": "Yao Hengshuai",
    "authors": "Yao HengShuai",
    "title": "Preconditioned Temporal Difference Learning",
    "comments": "This paper has been withdrawn by the author. Look at the ICML version\n  instead: http://icml2008.cs.helsinki.fi/papers/111.pdf",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": null,
    "abstract": "  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Apr 2007 13:17:01 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 12 Apr 2007 03:33:26 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 8 Jun 2012 14:08:19 GMT"
      }
    ],
    "update_date": "2012-06-11",
    "authors_parsed": [
      [
        "HengShuai",
        "Yao",
        ""
      ]
    ]
  },
  {
    "id": "0704.1675",
    "submitter": "Kristina Lerman",
    "authors": "Anon Plangprasopchok and Kristina Lerman",
    "title": "Exploiting Social Annotation for Automatic Resource Discovery",
    "comments": "6 pages, submitted to AAAI07 workshop on Information Integration on\n  the Web",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CY cs.DL",
    "license": null,
    "abstract": "  Information integration applications, such as mediators or mashups, that\nrequire access to information resources currently rely on users manually\ndiscovering and integrating them in the application. Manual resource discovery\nis a slow process, requiring the user to sift through results obtained via\nkeyword-based search. Although search methods have advanced to include evidence\nfrom document contents, its metadata and the contents and link structure of the\nreferring pages, they still do not adequately cover information sources --\noften called ``the hidden Web''-- that dynamically generate documents in\nresponse to a query. The recently popular social bookmarking sites, which allow\nusers to annotate and share metadata about various information sources, provide\nrich evidence for resource discovery. In this paper, we describe a\nprobabilistic model of the user annotation process in a social bookmarking\nsystem del.icio.us. We then use the model to automatically find resources\nrelevant to a particular information domain. Our experimental results on data\nobtained from \\emph{del.icio.us} show this approach as a promising method for\nhelping automate the resource discovery task.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 12 Apr 2007 23:24:19 GMT"
      }
    ],
    "update_date": "2016-09-08",
    "authors_parsed": [
      [
        "Plangprasopchok",
        "Anon",
        ""
      ],
      [
        "Lerman",
        "Kristina",
        ""
      ]
    ]
  },
  {
    "id": "0704.1676",
    "submitter": "Kristina Lerman",
    "authors": "Kristina Lerman, Anon Plangprasopchok and Chio Wong",
    "title": "Personalizing Image Search Results on Flickr",
    "comments": "12 pages, submitted to AAAI07 workshop on Intelligent Information\n  Personalization",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.IR cs.AI cs.CY cs.DL cs.HC",
    "license": null,
    "abstract": "  The social media site Flickr allows users to upload their photos, annotate\nthem with tags, submit them to groups, and also to form social networks by\nadding other users as contacts. Flickr offers multiple ways of browsing or\nsearching it. One option is tag search, which returns all images tagged with a\nspecific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an\ninsect or a car, tag search results will include many images that are not\nrelevant to the sense the user had in mind when executing the query. We claim\nthat users express their photography interests through the metadata they add in\nthe form of contacts and image annotations. We show how to exploit this\nmetadata to personalize search results for the user, thereby improving search\nperformance. First, we show that we can significantly improve search precision\nby filtering tag search results by user's contacts or a larger social network\nthat includes those contact's contacts. Secondly, we describe a probabilistic\nmodel that takes advantage of tag information to discover latent topics\ncontained in the search results. The users' interests can similarly be\ndescribed by the tags they used for annotating their images. The latent topics\nfound by the model are then used to personalize search results by finding\nimages on topics that are of interest to the user.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 12 Apr 2007 23:31:04 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Lerman",
        "Kristina",
        ""
      ],
      [
        "Plangprasopchok",
        "Anon",
        ""
      ],
      [
        "Wong",
        "Chio",
        ""
      ]
    ]
  },
  {
    "id": "0704.1783",
    "submitter": "Francesco Santini",
    "authors": "Stefano Bistarelli, Ugo Montanari, Francesca Rossi, Francesco Santini",
    "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming",
    "comments": "45 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LO cs.AI cs.NI",
    "license": null,
    "abstract": "  We present a formal model to represent and solve the unicast/multicast\nrouting problem in networks with Quality of Service (QoS) requirements. To\nattain this, first we translate the network adapting it to a weighted graph\n(unicast) or and-or graph (multicast), where the weight on a connector\ncorresponds to the multidimensional cost of sending a packet on the related\nnetwork link: each component of the weights vector represents a different QoS\nmetric value (e.g. bandwidth, cost, delay, packet loss). The second step\nconsists in writing this graph as a program in Soft Constraint Logic\nProgramming (SCLP): the engine of this framework is then able to find the best\npaths/trees by optimizing their costs and solving the constraints imposed on\nthem (e.g. delay < 40msec), thus finding a solution to QoS routing problems.\nMoreover, c-semiring structures are a convenient tool to model QoS metrics. At\nlast, we provide an implementation of the framework over scale-free networks\nand we suggest how the performance can be improved.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 13 Apr 2007 15:53:44 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 29 Apr 2007 15:40:10 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 21 Apr 2008 17:25:06 GMT"
      }
    ],
    "update_date": "2009-09-29",
    "authors_parsed": [
      [
        "Bistarelli",
        "Stefano",
        ""
      ],
      [
        "Montanari",
        "Ugo",
        ""
      ],
      [
        "Rossi",
        "Francesca",
        ""
      ],
      [
        "Santini",
        "Francesco",
        ""
      ]
    ]
  },
  {
    "id": "0704.2010",
    "submitter": "Juliana Bernardes",
    "authors": "Juliana S Bernardes, Alberto Davila, Vitor Santos Costa, Gerson\n  Zaverucha",
    "title": "A study of structural properties on profiles HMMs",
    "comments": "6 pages, 7 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 16 Apr 2007 13:10:35 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 11 Dec 2008 18:47:26 GMT"
      }
    ],
    "update_date": "2008-12-11",
    "authors_parsed": [
      [
        "Bernardes",
        "Juliana S",
        ""
      ],
      [
        "Davila",
        "Alberto",
        ""
      ],
      [
        "Costa",
        "Vitor Santos",
        ""
      ],
      [
        "Zaverucha",
        "Gerson",
        ""
      ]
    ]
  },
  {
    "id": "0704.2083",
    "submitter": "Hassan Satori",
    "authors": "H. Satori, M. Harti and N. Chenfour",
    "title": "Introduction to Arabic Speech Recognition Using CMUSphinx System",
    "comments": "4 pages, 3 figures and 2 tables, was in Information and Communication\n  Technologies International Symposium proceeding ICTIS07 Fes (2007)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": null,
    "abstract": "  In this paper Arabic was investigated from the speech recognition problem\npoint of view. We propose a novel approach to build an Arabic Automated Speech\nRecognition System (ASR). This system is based on the open source CMU Sphinx-4,\nfrom the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\nspeaker-independent, continuous speech recognition system based on discrete\nHidden Markov Models (HMMs). We build a model using utilities from the\nOpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\nsystem to Arabic voice recognition.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Apr 2007 01:04:01 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Satori",
        "H.",
        ""
      ],
      [
        "Harti",
        "M.",
        ""
      ],
      [
        "Chenfour",
        "N.",
        ""
      ]
    ]
  },
  {
    "id": "0704.2201",
    "submitter": "Hassan Satori",
    "authors": "H. Satori, M. Harti and N. Chenfour",
    "title": "Arabic Speech Recognition System using CMU-Sphinx4",
    "comments": "5 pages, 3 figures and 2 tables, in French",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": null,
    "abstract": "  In this paper we present the creation of an Arabic version of Automated\nSpeech Recognition System (ASR). This system is based on the open source\nSphinx-4, from the Carnegie Mellon University. Which is a speech recognition\nsystem based on discrete hidden Markov models (HMMs). We investigate the\nchanges that must be made to the model to adapt Arabic voice recognition.\n  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,\nCMUSphinx-4, Artificial intelligence.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Apr 2007 17:04:26 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Satori",
        "H.",
        ""
      ],
      [
        "Harti",
        "M.",
        ""
      ],
      [
        "Chenfour",
        "N.",
        ""
      ]
    ]
  },
  {
    "id": "0704.3157",
    "submitter": "Giorgio Terracina",
    "authors": "Giorgio Terracina, Nicola Leone, Vincenzino Lio, Claudio Panetta",
    "title": "Experimenting with recursive queries in database and logic programming\n  systems",
    "comments": "To appear in Theory and Practice of Logic Programming (TPLP)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.DB",
    "license": null,
    "abstract": "  This paper considers the problem of reasoning on massive amounts of (possibly\ndistributed) data. Presently, existing proposals show some limitations: {\\em\n(i)} the quantity of data that can be handled contemporarily is limited, due to\nthe fact that reasoning is generally carried out in main-memory; {\\em (ii)} the\ninteraction with external (and independent) DBMSs is not trivial and, in\nseveral cases, not allowed at all; {\\em (iii)} the efficiency of present\nimplementations is still not sufficient for their utilization in complex\nreasoning tasks involving massive amounts of data. This paper provides a\ncontribution in this setting; it presents a new system, called DLV$^{DB}$,\nwhich aims to solve these problems. Moreover, the paper reports the results of\na thorough experimental analysis we have carried out for comparing our system\nwith several state-of-the-art systems (both logic and databases) on some\nclassical deductive problems; the other tested systems are: LDL++, XSB, Smodels\nand three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even\nthe commercial Database Systems on recursive queries. To appear in Theory and\nPractice of Logic Programming (TPLP)\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 24 Apr 2007 10:58:40 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Terracina",
        "Giorgio",
        ""
      ],
      [
        "Leone",
        "Nicola",
        ""
      ],
      [
        "Lio",
        "Vincenzino",
        ""
      ],
      [
        "Panetta",
        "Claudio",
        ""
      ]
    ]
  },
  {
    "id": "0704.3359",
    "submitter": "Alex Smola J",
    "authors": "Quoc Le and Alexander Smola",
    "title": "Direct Optimization of Ranking Measures",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.IR cs.AI",
    "license": null,
    "abstract": "  Web page ranking and collaborative filtering require the optimization of\nsophisticated performance measures. Current Support Vector approaches are\nunable to optimize them directly and focus on pairwise comparisons instead. We\npresent a new approach which allows direct optimization of the relevant loss\nfunctions. This is achieved via structured estimation in Hilbert spaces. It is\nmost related to Max-Margin-Markov networks optimization of multivariate\nperformance measures. Key to our approach is that during training the ranking\nproblem can be viewed as a linear assignment problem, which can be solved by\nthe Hungarian Marriage algorithm. At test time, a sort operation is sufficient,\nas our algorithm assigns a relevance score to every (document, query) pair.\nExperiments show that the our algorithm is fast and that it works very well.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Apr 2007 12:36:55 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Le",
        "Quoc",
        ""
      ],
      [
        "Smola",
        "Alexander",
        ""
      ]
    ]
  },
  {
    "id": "0704.3395",
    "submitter": "Marko A. Rodriguez",
    "authors": "Marko A. Rodriguez",
    "title": "General-Purpose Computing on a Semantic Network Substrate",
    "comments": null,
    "journal-ref": "Emergent Web Intelligence: Advanced Semantic Technologies,\n  Advanced Information and Knowledge Processing series, Springer-Verlag, pages\n  57-104, ISBN:978-1-84996-076-2, June 2010",
    "doi": null,
    "report-no": "LA-UR-07-2885",
    "categories": "cs.AI cs.PL",
    "license": "http://creativecommons.org/licenses/publicdomain/",
    "abstract": "  This article presents a model of general-purpose computing on a semantic\nnetwork substrate. The concepts presented are applicable to any semantic\nnetwork representation. However, due to the standards and technological\ninfrastructure devoted to the Semantic Web effort, this article is presented\nfrom this point of view. In the proposed model of computing, the application\nprogramming interface, the run-time program, and the state of the computing\nvirtual machine are all represented in the Resource Description Framework\n(RDF). The implementation of the concepts presented provides a practical\ncomputing paradigm that leverages the highly-distributed and standardized\nrepresentational-layer of the Semantic Web.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Apr 2007 15:37:52 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 4 Oct 2007 20:08:21 GMT"
      },
      {
        "version": "v3",
        "created": "Sun, 7 Oct 2007 21:44:01 GMT"
      },
      {
        "version": "v4",
        "created": "Sun, 6 Jun 2010 05:29:22 GMT"
      }
    ],
    "update_date": "2010-06-08",
    "authors_parsed": [
      [
        "Rodriguez",
        "Marko A.",
        ""
      ]
    ]
  },
  {
    "id": "0704.3433",
    "submitter": "Tshilidzi Marwala",
    "authors": "Tshilidzi Marwala and Bodie Crossingham",
    "title": "Bayesian approach to rough set",
    "comments": "20 pages, 3 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper proposes an approach to training rough set models using Bayesian\nframework trained using Markov Chain Monte Carlo (MCMC) method. The prior\nprobabilities are constructed from the prior knowledge that good rough set\nmodels have fewer rules. Markov Chain Monte Carlo sampling is conducted through\nsampling in the rough set granule space and Metropolis algorithm is used as an\nacceptance criteria. The proposed method is tested to estimate the risk of HIV\ngiven demographic data. The results obtained shows that the proposed approach\nis able to achieve an average accuracy of 58% with the accuracy varying up to\n66%. In addition the Bayesian rough set give the probabilities of the estimated\nHIV status as well as the linguistic rules describing how the demographic\nparameters drive the risk of HIV.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Apr 2007 19:50:59 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Marwala",
        "Tshilidzi",
        ""
      ],
      [
        "Crossingham",
        "Bodie",
        ""
      ]
    ]
  },
  {
    "id": "0704.3453",
    "submitter": "Tshilidzi Marwala",
    "authors": "S. Mohamed, D. Rubin, and T. Marwala",
    "title": "An Adaptive Strategy for the Classification of G-Protein Coupled\n  Receptors",
    "comments": "9 pages, 5 tables, 3 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI q-bio.QM",
    "license": null,
    "abstract": "  One of the major problems in computational biology is the inability of\nexisting classification models to incorporate expanding and new domain\nknowledge. This problem of static classification models is addressed in this\npaper by the introduction of incremental learning for problems in\nbioinformatics. Many machine learning tools have been applied to this problem\nusing static machine learning structures such as neural networks or support\nvector machines that are unable to accommodate new information into their\nexisting models. We utilize the fuzzy ARTMAP as an alternate machine learning\nsystem that has the ability of incrementally learning new data as it becomes\navailable. The fuzzy ARTMAP is found to be comparable to many of the widespread\nmachine learning systems. The use of an evolutionary strategy in the selection\nand combination of individual classifiers into an ensemble system, coupled with\nthe incremental learning ability of the fuzzy ARTMAP is proven to be suitable\nas a pattern classifier. The algorithm presented is tested using data from the\nG-Coupled Protein Receptors Database and shows good accuracy of 83%. The system\npresented is also generally applicable, and can be used in problems in genomics\nand proteomics.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Apr 2007 21:23:31 GMT"
      }
    ],
    "update_date": "2007-06-25",
    "authors_parsed": [
      [
        "Mohamed",
        "S.",
        ""
      ],
      [
        "Rubin",
        "D.",
        ""
      ],
      [
        "Marwala",
        "T.",
        ""
      ]
    ]
  },
  {
    "id": "0704.3515",
    "submitter": "Jegor Uglov Mr",
    "authors": "J. Uglov, V. Schetinin, C. Maple",
    "title": "Comparing Robustness of Pairwise and Multiclass Neural-Network Systems\n  for Face Recognition",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1155/2008/468693",
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Noise, corruptions and variations in face images can seriously hurt the\nperformance of face recognition systems. To make such systems robust,\nmulticlass neuralnetwork classifiers capable of learning from noisy data have\nbeen suggested. However on large face data sets such systems cannot provide the\nrobustness at a high level. In this paper we explore a pairwise neural-network\nsystem as an alternative approach to improving the robustness of face\nrecognition. In our experiments this approach is shown to outperform the\nmulticlass neural-network system in terms of the predictive accuracy on the\nface images corrupted by noise.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 26 Apr 2007 11:29:19 GMT"
      }
    ],
    "update_date": "2016-02-17",
    "authors_parsed": [
      [
        "Uglov",
        "J.",
        ""
      ],
      [
        "Schetinin",
        "V.",
        ""
      ],
      [
        "Maple",
        "C.",
        ""
      ]
    ]
  },
  {
    "id": "0704.3886",
    "submitter": "W Saba",
    "authors": "Walid S. Saba",
    "title": "A Note on Ontology and Ordinary Language",
    "comments": "19 pages, 1 figure",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CL",
    "license": null,
    "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology\nthat reflects our commonsense view of the world and the way we talk about it.\nAssuming such a structure we show that the semantics of various natural\nlanguage phenomena may become nearly trivial.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 30 Apr 2007 17:55:39 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 1 May 2007 13:43:32 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 2 May 2007 18:13:22 GMT"
      },
      {
        "version": "v4",
        "created": "Thu, 3 May 2007 08:34:47 GMT"
      },
      {
        "version": "v5",
        "created": "Fri, 4 May 2007 17:49:03 GMT"
      },
      {
        "version": "v6",
        "created": "Mon, 7 May 2007 16:04:50 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Saba",
        "Walid S.",
        ""
      ]
    ]
  },
  {
    "id": "0704.3905",
    "submitter": "Marc Schoenauer",
    "authors": "Christian Gagn\\'e (INFORMATIQUE WGZ INC.), Mich\\`ele Sebag (INRIA\n  Futurs), Marc Schoenauer (INRIA Futurs), Marco Tomassini (ISI)",
    "title": "Ensemble Learning for Free with Evolutionary Algorithms ?",
    "comments": null,
    "journal-ref": "Dans GECCO (2007)",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Evolutionary Learning proceeds by evolving a population of classifiers, from\nwhich it generally returns (with some notable exceptions) the single\nbest-of-run classifier as final result. In the meanwhile, Ensemble Learning,\none of the most efficient approaches in supervised Machine Learning for the\nlast decade, proceeds by building a population of diverse classifiers. Ensemble\nLearning with Evolutionary Computation thus receives increasing attention. The\nEvolutionary Ensemble Learning (EEL) approach presented in this paper features\ntwo contributions. First, a new fitness function, inspired by co-evolution and\nenforcing the classifier diversity, is presented. Further, a new selection\ncriterion based on the classification margin is proposed. This criterion is\nused to extract the classifier ensemble from the final population only\n(Off-line) or incrementally along evolution (On-line). Experiments on a set of\nbenchmark problems show that Off-line outperforms single-hypothesis\nevolutionary learning and state-of-art Boosting and generates smaller\nclassifier ensembles.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 30 Apr 2007 09:29:22 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Gagné",
        "Christian",
        "",
        "INFORMATIQUE WGZ INC."
      ],
      [
        "Sebag",
        "Michèle",
        "",
        "INRIA\n  Futurs"
      ],
      [
        "Schoenauer",
        "Marc",
        "",
        "INRIA Futurs"
      ],
      [
        "Tomassini",
        "Marco",
        "",
        "ISI"
      ]
    ]
  },
  {
    "id": "0705.0025",
    "submitter": "Andreas Martin Lisewski",
    "authors": "Andreas Martin Lisewski",
    "title": "Can the Internet cope with stress?",
    "comments": "4 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.HC cs.AI",
    "license": null,
    "abstract": "  When will the Internet become aware of itself? In this note the problem is\napproached by asking an alternative question: Can the Internet cope with\nstress? By extrapolating the psychological difference between coping and\ndefense mechanisms a distributed software experiment is outlined which could\nreject the hypothesis that the Internet is not a conscious entity.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 1 May 2007 15:44:17 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Lisewski",
        "Andreas Martin",
        ""
      ]
    ]
  },
  {
    "id": "0705.0197",
    "submitter": "Tshilidzi Marwala",
    "authors": "Tshilidzi Marwala, Unathi Mahola and Snehashish Chakraverty",
    "title": "Fault Classification in Cylinders Using Multilayer Perceptrons, Support\n  Vector Machines and Guassian Mixture Models",
    "comments": "10 pages, 2 figures, 4 tables",
    "journal-ref": "Computer Assisted Mechanics and Engineering Sciences, Vol. 14, No.\n  2, 2007.",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Gaussian mixture models (GMM) and support vector machines (SVM) are\nintroduced to classify faults in a population of cylindrical shells. The\nproposed procedures are tested on a population of 20 cylindrical shells and\ntheir performance is compared to the procedure, which uses multi-layer\nperceptrons (MLP). The modal properties extracted from vibration data are used\nto train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM\nproduces 94% classification accuracy while the MLP produces 88% classification\nrates.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 2 May 2007 03:13:28 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Marwala",
        "Tshilidzi",
        ""
      ],
      [
        "Mahola",
        "Unathi",
        ""
      ],
      [
        "Chakraverty",
        "Snehashish",
        ""
      ]
    ]
  },
  {
    "id": "0705.0199",
    "submitter": "Erik Berglund",
    "authors": "Erik Berglund, Joaquin Sitte",
    "title": "The Parameter-Less Self-Organizing Map algorithm",
    "comments": "29 pages, 27 figures. Based on publication in IEEE Trans. on Neural\n  Networks",
    "journal-ref": "IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316",
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI cs.CV",
    "license": null,
    "abstract": "  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network\nalgorithm based on the Self-Organizing Map (SOM). It eliminates the need for a\nlearning rate and annealing schemes for learning rate and neighbourhood size.\nWe discuss the relative performance of the PLSOM and the SOM and demonstrate\nsome tasks in which the SOM fails but the PLSOM performs satisfactory. Finally\nwe discuss some example applications of the PLSOM and present a proof of\nordering under certain limited conditions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 2 May 2007 04:04:51 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 8 May 2007 01:06:10 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Berglund",
        "Erik",
        ""
      ],
      [
        "Sitte",
        "Joaquin",
        ""
      ]
    ]
  },
  {
    "id": "0705.0588",
    "submitter": "Edgar Graaf de",
    "authors": "Edgar H. de Graaf, Joost N. Kok, Walter A. Kosters",
    "title": "Clustering Co-occurrence of Maximal Frequent Patterns in Streams",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.DS",
    "license": null,
    "abstract": "  One way of getting a better view of data is using frequent patterns. In this\npaper frequent patterns are subsets that occur a minimal number of times in a\nstream of itemsets. However, the discovery of frequent patterns in streams has\nalways been problematic. Because streams are potentially endless it is in\nprinciple impossible to say if a pattern is often occurring or not. Furthermore\nthe number of patterns can be huge and a good overview of the structure of the\nstream is lost quickly. The proposed approach will use clustering to facilitate\nthe analysis of the structure of the stream.\n  A clustering on the co-occurrence of patterns will give the user an improved\nview on the structure of the stream. Some patterns might occur so much together\nthat they should form a combined pattern. In this way the patterns in the\nclustering will be the largest frequent patterns: maximal frequent patterns.\n  Our approach to decide if patterns occur often together will be based on a\nmethod of clustering when only the distance between pairs is known. The number\nof maximal frequent patterns is much smaller and combined with clustering\nmethods these patterns provide a good view on the structure of the stream.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 4 May 2007 10:36:53 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "de Graaf",
        "Edgar H.",
        ""
      ],
      [
        "Kok",
        "Joost N.",
        ""
      ],
      [
        "Kosters",
        "Walter A.",
        ""
      ]
    ]
  },
  {
    "id": "0705.0593",
    "submitter": "Edgar Graaf de",
    "authors": "Edgar H. de Graaf, Joost N. Kok, Walter A. Kosters",
    "title": "Clustering with Lattices in the Analysis of Graph Patterns",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.DS",
    "license": null,
    "abstract": "  Mining frequent subgraphs is an area of research where we have a given set of\ngraphs (each graph can be seen as a transaction), and we search for (connected)\nsubgraphs contained in many of these graphs. In this work we will discuss\ntechniques used in our framework Lattice2SAR for mining and analysing frequent\nsubgraph data and their corresponding lattice information. Lattice information\nis provided by the graph mining algorithm gSpan; it contains all\nsupergraph-subgraph relations of the frequent subgraph patterns -- and their\nsupports.\n  Lattice2SAR is in particular used in the analysis of frequent graph patterns\nwhere the graphs are molecules and the frequent subgraphs are fragments. In the\nanalysis of fragments one is interested in the molecules where patterns occur.\nThis data can be very extensive and in this paper we focus on a technique of\nmaking it better available by using the lattice information in our clustering.\nNow we can reduce the number of times the highly compressed occurrence data\nneeds to be accessed by the user. The user does not have to browse all the\noccurrence data in search of patterns occurring in the same molecules. Instead\none can directly see which frequent subgraphs are of interest.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 4 May 2007 10:52:28 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "de Graaf",
        "Edgar H.",
        ""
      ],
      [
        "Kok",
        "Joost N.",
        ""
      ],
      [
        "Kosters",
        "Walter A.",
        ""
      ]
    ]
  },
  {
    "id": "0705.0693",
    "submitter": "Tshilidzi Marwala",
    "authors": "Evan Hurwitz and Tshilidzi Marwala",
    "title": "Learning to Bluff",
    "comments": "6 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  The act of bluffing confounds game designers to this day. The very nature of\nbluffing is even open for debate, adding further complication to the process of\ncreating intelligent virtual players that can bluff, and hence play,\nrealistically. Through the use of intelligent, learning agents, and carefully\ndesigned agent outlooks, an agent can in fact learn to predict its opponents\nreactions based not only on its own cards, but on the actions of those around\nit. With this wider scope of understanding, an agent can in learn to bluff its\nopponents, with the action representing not an illogical action, as bluffing is\noften viewed, but rather as an act of maximising returns through an effective\nstatistical optimisation. By using a tee dee lambda learning algorithm to\ncontinuously adapt neural network agent intelligence, agents have been shown to\nbe able to learn to bluff without outside prompting, and even to learn to call\neach others bluffs in free, competitive play.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 May 2007 19:15:24 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Hurwitz",
        "Evan",
        ""
      ],
      [
        "Marwala",
        "Tshilidzi",
        ""
      ]
    ]
  },
  {
    "id": "0705.0734",
    "submitter": "Sanjiang Li",
    "authors": "Sanjiang Li and Mingsheng Ying",
    "title": "Soft constraint abstraction based on semiring homomorphism",
    "comments": "18 pages, 1 figure",
    "journal-ref": "Theoretical Computer Science 403(2-3) 192-201, 2008",
    "doi": "10.1016/j.tcs.2008.03.029",
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  The semiring-based constraint satisfaction problems (semiring CSPs), proposed\nby Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of\nsoft constraints. In this paper we propose an abstraction scheme for soft\nconstraints that uses semiring homomorphism. To find optimal solutions of the\nconcrete problem, the idea is, first working in the abstract problem and\nfinding its optimal solutions, then using them to solve the concrete problem.\n  In particular, we show that a mapping preserves optimal solutions if and only\nif it is an order-reflecting semiring homomorphism. Moreover, for a semiring\nhomomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in\n$\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that\n$\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 5 May 2007 08:47:31 GMT"
      }
    ],
    "update_date": "2010-07-01",
    "authors_parsed": [
      [
        "Li",
        "Sanjiang",
        ""
      ],
      [
        "Ying",
        "Mingsheng",
        ""
      ]
    ]
  },
  {
    "id": "0705.0760",
    "submitter": "Sujay Sanghavi",
    "authors": "Sujay Sanghavi",
    "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs",
    "comments": "6 pages, 2 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.IT cs.AI cs.LG cs.NI math.IT",
    "license": null,
    "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 5 May 2007 18:57:47 GMT"
      }
    ],
    "update_date": "2007-07-13",
    "authors_parsed": [
      [
        "Sanghavi",
        "Sujay",
        ""
      ]
    ]
  },
  {
    "id": "0705.0761",
    "submitter": "Tshilidzi Marwala",
    "authors": "Tshilidzi Marwala and Bodie Crossingham",
    "title": "Bayesian Approach to Neuro-Rough Models",
    "comments": "24 pages, 5 figures, 1 table",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper proposes a neuro-rough model based on multi-layered perceptron and\nrough set. The neuro-rough model is then tested on modelling the risk of HIV\nfrom demographic data. The model is formulated using Bayesian framework and\ntrained using Monte Carlo method and Metropolis criterion. When the model was\ntested to estimate the risk of HIV infection given the demographic data it was\nfound to give the accuracy of 62%. The proposed model is able to combine the\naccuracy of the Bayesian MLP model and the transparency of Bayesian rough set\nmodel.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 6 May 2007 22:55:58 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 9 May 2007 04:13:04 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 28 Aug 2007 09:24:46 GMT"
      }
    ],
    "update_date": "2007-08-28",
    "authors_parsed": [
      [
        "Marwala",
        "Tshilidzi",
        ""
      ],
      [
        "Crossingham",
        "Bodie",
        ""
      ]
    ]
  },
  {
    "id": "0705.0969",
    "submitter": "Tshilidzi Marwala",
    "authors": "Ishmael S. Msiza, Fulufhelo V. Nelwamondo and Tshilidzi Marwala",
    "title": "Artificial Neural Networks and Support Vector Machines for Water Demand\n  Time Series Forecasting",
    "comments": "6 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Water plays a pivotal role in many physical processes, and most importantly\nin sustaining human life, animal life and plant life. Water supply entities\ntherefore have the responsibility to supply clean and safe water at the rate\nrequired by the consumer. It is therefore necessary to implement mechanisms and\nsystems that can be employed to predict both short-term and long-term water\ndemands. The increasingly growing field of computational intelligence\ntechniques has been proposed as an efficient tool in the modelling of dynamic\nphenomena. The primary objective of this paper is to compare the efficiency of\ntwo computational intelligence techniques in water demand forecasting. The\ntechniques under comparison are the Artificial Neural Networks (ANNs) and the\nSupport Vector Machines (SVMs). In this study it was observed that the ANNs\nperform better than the SVMs. This performance is measured against the\ngeneralisation ability of the two.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 May 2007 19:00:28 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Msiza",
        "Ishmael S.",
        ""
      ],
      [
        "Nelwamondo",
        "Fulufhelo V.",
        ""
      ],
      [
        "Marwala",
        "Tshilidzi",
        ""
      ]
    ]
  },
  {
    "id": "0705.1031",
    "submitter": "Tshilidzi Marwala",
    "authors": "F.V. Nelwamondo and T. Marwala",
    "title": "Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs\n  with Missing Values",
    "comments": "7 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  An ensemble based approach for dealing with missing data, without predicting\nor imputing the missing values is proposed. This technique is suitable for\nonline operations of neural networks and as a result, is used for online\ncondition monitoring. The proposed technique is tested in both classification\nand regression problems. An ensemble of Fuzzy-ARTMAPs is used for\nclassification whereas an ensemble of multi-layer perceptrons is used for the\nregression problem. Results obtained using this ensemble-based technique are\ncompared to those obtained using a combination of auto-associative neural\nnetworks and genetic algorithms and findings show that this method can perform\nup to 9% better in regression problems. Another advantage of the proposed\ntechnique is that it eliminates the need for finding the best estimate of the\ndata, and hence, saves time.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 8 May 2007 05:12:01 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Nelwamondo",
        "F. V.",
        ""
      ],
      [
        "Marwala",
        "T.",
        ""
      ]
    ]
  },
  {
    "id": "0705.1110",
    "submitter": "Edgar Graaf de",
    "authors": "Edgar de Graaf Joost Kok Walter Kosters",
    "title": "Mining Patterns with a Balanced Interval",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.DB",
    "license": null,
    "abstract": "  In many applications it will be useful to know those patterns that occur with\na balanced interval, e.g., a certain combination of phone numbers are called\nalmost every Friday or a group of products are sold a lot on Tuesday and\nThursday.\n  In previous work we proposed a new measure of support (the number of\noccurrences of a pattern in a dataset), where we count the number of times a\npattern occurs (nearly) in the middle between two other occurrences. If the\nnumber of non-occurrences between two occurrences of a pattern stays almost the\nsame then we call the pattern balanced.\n  It was noticed that some very frequent patterns obviously also occur with a\nbalanced interval, meaning in every transaction. However more interesting\npatterns might occur, e.g., every three transactions. Here we discuss a\nsolution using standard deviation and average. Furthermore we propose a simpler\napproach for pruning patterns with a balanced interval, making estimating the\npruning threshold more intuitive.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 8 May 2007 15:22:38 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Kosters",
        "Edgar de Graaf Joost Kok Walter",
        ""
      ]
    ]
  },
  {
    "id": "0705.1209",
    "submitter": "Tshilidzi Marwala",
    "authors": "E. Habtemariam, T. Marwala and M. Lagazio",
    "title": "Artificial Intelligence for Conflict Management",
    "comments": "20 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Militarised conflict is one of the risks that have a significant impact on\nsociety. Militarised Interstate Dispute (MID) is defined as an outcome of\ninterstate interactions, which result on either peace or conflict. Effective\nprediction of the possibility of conflict between states is an important\ndecision support tool for policy makers. In a previous research, neural\nnetworks (NNs) have been implemented to predict the MID. Support Vector\nMachines (SVMs) have proven to be very good prediction techniques and are\nintroduced for the prediction of MIDs in this study and compared to neural\nnetworks. The results show that SVMs predict MID better than NNs while NNs give\nmore consistent and easy to interpret sensitivity analysis than SVMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 9 May 2007 05:53:30 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Habtemariam",
        "E.",
        ""
      ],
      [
        "Marwala",
        "T.",
        ""
      ],
      [
        "Lagazio",
        "M.",
        ""
      ]
    ]
  },
  {
    "id": "0705.1244",
    "submitter": "Marc Schoenauer",
    "authors": "Nicolas Godzik (INRIA Futurs, INRIA Rocquencourt), Marc Schoenauer\n  (INRIA Futurs, INRIA Rocquencourt), Mich\\`ele Sebag (INRIA Futurs, LRI)",
    "title": "Evolving Symbolic Controllers",
    "comments": null,
    "journal-ref": "Dans 4th European Workshop on Evolutionary Robotics, 2611 (2003)\n  638-650",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  The idea of symbolic controllers tries to bridge the gap between the top-down\nmanual design of the controller architecture, as advocated in Brooks'\nsubsumption architecture, and the bottom-up designer-free approach that is now\nstandard within the Evolutionary Robotics community. The designer provides a\nset of elementary behavior, and evolution is given the goal of assembling them\nto solve complex tasks. Two experiments are presented, demonstrating the\nefficiency and showing the recursiveness of this approach. In particular, the\nsensitivity with respect to the proposed elementary behaviors, and the\nrobustness w.r.t. generalization of the resulting controllers are studied in\ndetail.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 9 May 2007 09:53:31 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Godzik",
        "Nicolas",
        "",
        "INRIA Futurs, INRIA Rocquencourt"
      ],
      [
        "Schoenauer",
        "Marc",
        "",
        "INRIA Futurs, INRIA Rocquencourt"
      ],
      [
        "Sebag",
        "Michèle",
        "",
        "INRIA Futurs, LRI"
      ]
    ]
  },
  {
    "id": "0705.1309",
    "submitter": "Marc Schoenauer",
    "authors": "Alexandre Devert (INRIA Futurs), Nicolas Bred\\`eche (INRIA Futurs),\n  Marc Schoenauer (INRIA Futurs)",
    "title": "Robust Multi-Cellular Developmental Design",
    "comments": null,
    "journal-ref": "Dans Genetic and Evolutionary Computation COnference (2007)",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper introduces a continuous model for Multi-cellular Developmental\nDesign. The cells are fixed on a 2D grid and exchange \"chemicals\" with their\nneighbors during the growth process. The quantity of chemicals that a cell\nproduces, as well as the differentiation value of the cell in the phenotype,\nare controlled by a Neural Network (the genotype) that takes as inputs the\nchemicals produced by the neighboring cells at the previous time step. In the\nproposed model, the number of iterations of the growth process is not\npre-determined, but emerges during evolution: only organisms for which the\ngrowth process stabilizes give a phenotype (the stable state), others are\ndeclared nonviable. The optimization of the controller is done using the NEAT\nalgorithm, that optimizes both the topology and the weights of the Neural\nNetworks. Though each cell only receives local information from its neighbors,\nthe experimental results of the proposed approach on the 'flags' problems (the\nphenotype must match a given 2D pattern) are almost as good as those of a\ndirect regression approach using the same model with global information.\nMoreover, the resulting multi-cellular organisms exhibit almost perfect\nself-healing characteristics.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 9 May 2007 15:33:34 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Devert",
        "Alexandre",
        "",
        "INRIA Futurs"
      ],
      [
        "Bredèche",
        "Nicolas",
        "",
        "INRIA Futurs"
      ],
      [
        "Schoenauer",
        "Marc",
        "",
        "INRIA Futurs"
      ]
    ]
  },
  {
    "id": "0705.1617",
    "submitter": "Daegene Song",
    "authors": "Daegene Song",
    "title": "Non-Computability of Consciousness",
    "comments": "10 pages, 2 figures, 1 table",
    "journal-ref": "NeuroQuantology 5, 382 (2007).",
    "doi": null,
    "report-no": null,
    "categories": "quant-ph astro-ph cs.AI",
    "license": null,
    "abstract": "  With the great success in simulating many intelligent behaviors using\ncomputing devices, there has been an ongoing debate whether all conscious\nactivities are computational processes. In this paper, the answer to this\nquestion is shown to be no. A certain phenomenon of consciousness is\ndemonstrated to be fully represented as a computational process using a quantum\ncomputer. Based on the computability criterion discussed with Turing machines,\nthe model constructed is shown to necessarily involve a non-computable element.\nThe concept that this is solely a quantum effect and does not work for a\nclassical case is also discussed.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 11 May 2007 10:16:48 GMT"
      }
    ],
    "update_date": "2011-11-09",
    "authors_parsed": [
      [
        "Song",
        "Daegene",
        ""
      ]
    ]
  },
  {
    "id": "0705.1673",
    "submitter": "Tshilidzi Marwala",
    "authors": "L. Mdlazi, C.J. Stander, P.S. Heyns and T. Marwala",
    "title": "Using artificial intelligence for data reduction in mechanical\n  engineering",
    "comments": "6 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CE cs.AI cs.NE",
    "license": null,
    "abstract": "  In this paper artificial neural networks and support vector machines are used\nto reduce the amount of vibration data that is required to estimate the Time\nDomain Average of a gear vibration signal. Two models for estimating the time\ndomain average of a gear vibration signal are proposed. The models are tested\non data from an accelerated gear life test rig. Experimental results indicate\nthat the required data for calculating the Time Domain Average of a gear\nvibration signal can be reduced by up to 75% when the proposed models are\nimplemented.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 11 May 2007 15:49:40 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Mdlazi",
        "L.",
        ""
      ],
      [
        "Stander",
        "C. J.",
        ""
      ],
      [
        "Heyns",
        "P. S.",
        ""
      ],
      [
        "Marwala",
        "T.",
        ""
      ]
    ]
  },
  {
    "id": "0705.1999",
    "submitter": "Camilla Schwind",
    "authors": "Camilla Schwind (LIF)",
    "title": "A first-order Temporal Logic for Actions",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LO",
    "license": null,
    "abstract": "  We present a multi-modal action logic with first-order modalities, which\ncontain terms which can be unified with the terms inside the subsequent\nformulas and which can be quantified. This makes it possible to handle\nsimultaneously time and states. We discuss applications of this language to\naction theory where it is possible to express many temporal aspects of actions,\nas for example, beginning, end, time points, delayed preconditions and results,\nduration and many others. We present tableaux rules for a decidable fragment of\nthis logic.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 May 2007 18:36:25 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Schwind",
        "Camilla",
        "",
        "LIF"
      ]
    ]
  },
  {
    "id": "0705.2011",
    "submitter": "Alex Graves",
    "authors": "Alex Graves, Santiago Fernandez, Juergen Schmidhuber",
    "title": "Multi-Dimensional Recurrent Neural Networks",
    "comments": "10 pages, 10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": "04-07",
    "categories": "cs.AI cs.CV",
    "license": null,
    "abstract": "  Recurrent neural networks (RNNs) have proved effective at one dimensional\nsequence learning tasks, such as speech and online handwriting recognition.\nSome of the properties that make RNNs suitable for such tasks, for example\nrobustness to input warping, and the ability to access contextual information,\nare also desirable in multidimensional domains. However, there has so far been\nno direct way of applying RNNs to data with more than one spatio-temporal\ndimension. This paper introduces multi-dimensional recurrent neural networks\n(MDRNNs), thereby extending the potential applicability of RNNs to vision,\nvideo processing, medical imaging and many other areas, while avoiding the\nscaling problems that have plagued other multi-dimensional models. Experimental\nresults are provided for two image segmentation tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 May 2007 19:49:56 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Graves",
        "Alex",
        ""
      ],
      [
        "Fernandez",
        "Santiago",
        ""
      ],
      [
        "Schmidhuber",
        "Juergen",
        ""
      ]
    ]
  },
  {
    "id": "0705.2235",
    "submitter": "Tshilidzi Marwala",
    "authors": "S. Chakraverty, T. Marwala, Pallavi Gupta and Thando Tettey",
    "title": "Response Prediction of Structural System Subject to Earthquake Motions\n  using Artificial Neural Network",
    "comments": "18 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper uses Artificial Neural Network (ANN) models to compute response of\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi\nground motion data. The system is first trained for a single real earthquake\ndata. The trained ANN architecture is then used to simulate earthquakes with\nvarious intensities and it was found that the predicted responses given by ANN\nmodel are accurate for practical purposes. When the ANN is trained by a part of\nthe ground motion data, it can also identify the responses of the structural\nsystem well. In this way the safeness of the structural systems may be\npredicted in case of future earthquakes without waiting for the earthquake to\noccur for the lessons. Time period and the corresponding maximum response of\nthe building for an earthquake has been evaluated, which is again trained to\npredict the maximum response of the building at different time periods. The\ntrained time period versus maximum response ANN model is also tested for real\nearthquake data of other place, which was not used in the training and was\nfound to be in good agreement.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 15 May 2007 20:29:06 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Chakraverty",
        "S.",
        ""
      ],
      [
        "Marwala",
        "T.",
        ""
      ],
      [
        "Gupta",
        "Pallavi",
        ""
      ],
      [
        "Tettey",
        "Thando",
        ""
      ]
    ]
  },
  {
    "id": "0705.2236",
    "submitter": "Tshilidzi Marwala",
    "authors": "Tshilidzi Marwala, Thando Tettey and Snehashish Chakraverty",
    "title": "Fault Classification using Pseudomodal Energies and Neuro-fuzzy\n  modelling",
    "comments": "8 pages, In Proceedings of the Asia-Pacific Workshop on Structural\n  Health Monitoring, Yokohama, Japan, 2006",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper presents a fault classification method which makes use of a\nTakagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the\nvibration signals of cylindrical shells. The calculation of Pseudomodal\nEnergies, for the purposes of condition monitoring, has previously been found\nto be an accurate method of extracting features from vibration signals. This\ncalculation is therefore used to extract features from vibration signals\nobtained from a diverse population of cylindrical shells. Some of the cylinders\nin the population have faults in different substructures. The pseudomodal\nenergies calculated from the vibration signals are then used as inputs to a\nneuro-fuzzy model. A leave-one-out cross-validation process is used to test the\nperformance of the model. It is found that the neuro-fuzzy model is able to\nclassify faults with an accuracy of 91.62%, which is higher than the previously\nused multilayer perceptron.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 15 May 2007 20:34:05 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Marwala",
        "Tshilidzi",
        ""
      ],
      [
        "Tettey",
        "Thando",
        ""
      ],
      [
        "Chakraverty",
        "Snehashish",
        ""
      ]
    ]
  },
  {
    "id": "0705.2305",
    "submitter": "Tshilidzi Marwala",
    "authors": "Sizwe M. Dhlamini, Tshilidzi Marwala, and Thokozani Majozi",
    "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings",
    "comments": "7 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.NE",
    "license": null,
    "abstract": "  The work proposes the application of fuzzy set theory (FST) to diagnose the\ncondition of high voltage bushings. The diagnosis uses dissolved gas analysis\n(DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil\nimpregnated paper (OIP) bushings. FST and neural networks are compared in terms\nof accuracy and computational efficiency. Both FST and NN simulations were able\nto diagnose the bushings condition with 10% error. By using fuzzy theory, the\nmaintenance department can classify bushings and know the extent of degradation\nin the component.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 16 May 2007 09:06:19 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Dhlamini",
        "Sizwe M.",
        ""
      ],
      [
        "Marwala",
        "Tshilidzi",
        ""
      ],
      [
        "Majozi",
        "Thokozani",
        ""
      ]
    ]
  },
  {
    "id": "0705.2307",
    "submitter": "Tshilidzi Marwala",
    "authors": "Bradley van Aardt, Tshilidzi Marwala",
    "title": "A Study in a Hybrid Centralised-Swarm Agent Community",
    "comments": "6 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI",
    "license": null,
    "abstract": "  This paper describes a systems architecture for a hybrid Centralised/Swarm\nbased multi-agent system. The issue of local goal assignment for agents is\ninvestigated through the use of a global agent which teaches the agents\nresponses to given situations. We implement a test problem in the form of a\nPursuit game, where the Multi-Agent system is a set of captor agents. The\nagents learn solutions to certain board positions from the global agent if they\nare unable to find a solution. The captor agents learn through the use of\nmulti-layer perceptron neural networks. The global agent is able to solve board\npositions through the use of a Genetic Algorithm. The cooperation between\nagents and the results of the simulation are discussed here. .\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 16 May 2007 09:12:09 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "van Aardt",
        "Bradley",
        ""
      ],
      [
        "Marwala",
        "Tshilidzi",
        ""
      ]
    ]
  },
  {
    "id": "0705.2310",
    "submitter": "Tshilidzi Marwala",
    "authors": "C.B. Vilakazi, T. Marwala, P. Mautla and E. Moloto",
    "title": "On-Line Condition Monitoring using Computational Intelligence",
    "comments": "8 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper presents bushing condition monitoring frameworks that use\nmulti-layer perceptrons (MLP), radial basis functions (RBF) and support vector\nmachines (SVM) classifiers. The first level of the framework determines if the\nbushing is faulty or not while the second level determines the type of fault.\nThe diagnostic gases in the bushings are analyzed using the dissolve gas\nanalysis. MLP gives superior performance in terms of accuracy and training time\nthan SVM and RBF. In addition, an on-line bushing condition monitoring\napproach, which is able to adapt to newly acquired data are introduced. This\napproach is able to accommodate new classes that are introduced by incoming\ndata and is implemented using an incremental learning algorithm that uses MLP.\nThe testing results improved from 67.5% to 95.8% as new data were introduced\nand the testing results improved from 60% to 95.3% as new conditions were\nintroduced. On average the confidence value of the framework on its decision\nwas 0.92.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 16 May 2007 09:19:00 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Vilakazi",
        "C. B.",
        ""
      ],
      [
        "Marwala",
        "T.",
        ""
      ],
      [
        "Mautla",
        "P.",
        ""
      ],
      [
        "Moloto",
        "E.",
        ""
      ]
    ]
  },
  {
    "id": "0705.2485",
    "submitter": "Bodie Crossingham",
    "authors": "Bodie Crossingham and Tshilidzi Marwala",
    "title": "Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV\n  Data Analysis",
    "comments": "10 pages, 1 figure, Update Bibliography",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI q-bio.QM",
    "license": null,
    "abstract": "  In this paper, we present a method to optimise rough set partition sizes, to\nwhich rule extraction is performed on HIV data. The genetic algorithm\noptimisation technique is used to determine the partition sizes of a rough set\nin order to maximise the rough sets prediction accuracy. The proposed method is\ntested on a set of demographic properties of individuals obtained from the\nSouth African antenatal survey. Six demographic variables were used in the\nanalysis, these variables are; race, age of mother, education, gravidity,\nparity, and age of father, with the outcome or decision being either HIV\npositive or negative. Rough set theory is chosen based on the fact that it is\neasy to interpret the extracted rules. The prediction accuracy of equal width\nbin partitioning is 57.7% while the accuracy achieved after optimising the\npartitions is 72.8%. Several other methods have been used to analyse the HIV\ndata and their results are stated and compared to that of rough set theory\n(RST).\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 17 May 2007 07:02:23 GMT"
      }
    ],
    "update_date": "2007-06-25",
    "authors_parsed": [
      [
        "Crossingham",
        "Bodie",
        ""
      ],
      [
        "Marwala",
        "Tshilidzi",
        ""
      ]
    ]
  },
  {
    "id": "0705.2516",
    "submitter": "Tshilidzi Marwala",
    "authors": "Sizwe M. Dhlamini*, Fulufhelo V. Nelwamondo**, Tshilidzi Marwala**",
    "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data\n  Using Evolutionary Computing",
    "comments": "7 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI",
    "license": null,
    "abstract": "  The work proposes the application of neural networks with particle swarm\noptimisation (PSO) and genetic algorithms (GA) to compensate for missing data\nin classifying high voltage bushings. The classification is done using DGA data\nfrom 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates\nmethods for oil impregnated paper (OIP) bushings. PSO and GA were compared in\nterms of accuracy and computational efficiency. Both GA and PSO simulations\nwere able to estimate missing data values to an average 95% accuracy when only\none variable was missing. However PSO rapidly deteriorated to 66% accuracy with\ntwo variables missing simultaneously, compared to 84% for GA. The data\nestimated using GA was found to classify the conditions of bushings than the\nPSO.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 17 May 2007 11:33:34 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Dhlamini*",
        "Sizwe M.",
        ""
      ],
      [
        "Nelwamondo**",
        "Fulufhelo V.",
        ""
      ],
      [
        "Marwala**",
        "Tshilidzi",
        ""
      ]
    ]
  },
  {
    "id": "0705.2765",
    "submitter": "Rustem Takhanov",
    "authors": "Rustem Takhanov",
    "title": "On the monotonization of the training set",
    "comments": "10 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": null,
    "abstract": "  We consider the problem of minimal correction of the training set to make it\nconsistent with monotonic constraints. This problem arises during analysis of\ndata sets via techniques that require monotone data. We show that this problem\nis NP-hard in general and is equivalent to finding a maximal independent set in\nspecial orgraphs. Practically important cases of that problem considered in\ndetail. These are the cases when a partial order given on the replies set is a\ntotal order or has a dimension 2. We show that the second case can be reduced\nto maximization of a quadratic convex function on a convex set. For this case\nwe construct an approximate polynomial algorithm based on convex optimization.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 18 May 2007 19:44:19 GMT"
      }
    ],
    "update_date": "2007-05-23",
    "authors_parsed": [
      [
        "Takhanov",
        "Rustem",
        ""
      ]
    ]
  },
  {
    "id": "0705.3360",
    "submitter": "Kyriakos Sgarbas",
    "authors": "Kyriakos N. Sgarbas",
    "title": "The Road to Quantum Artificial Intelligence",
    "comments": "9 pages. Presented at PCI-2007: 11th Panhellenic Conference in\n  Informatics, 18-20 May 2007, Patras, Greece",
    "journal-ref": "In: T.S.Papatheodorou, D.N.Christodoulakis and N.N.Karanikolas\n  (eds), \"Current Trends in Informatics\", Vol.A, pp.469-477, New Technologies\n  Publications, Athens, 2007 (SET 978-960-89784-0-9)",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper overviews the basic principles and recent advances in the emerging\nfield of Quantum Computation (QC), highlighting its potential application to\nArtificial Intelligence (AI). The paper provides a very brief introduction to\nbasic QC issues like quantum registers, quantum gates and quantum algorithms\nand then it presents references, ideas and research guidelines on how QC can be\nused to deal with some basic AI problems, such as search and pattern matching,\nas soon as quantum computers become widely available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 23 May 2007 12:31:47 GMT"
      }
    ],
    "update_date": "2007-05-24",
    "authors_parsed": [
      [
        "Sgarbas",
        "Kyriakos N.",
        ""
      ]
    ]
  },
  {
    "id": "0705.3561",
    "submitter": "Lucas Bordeaux",
    "authors": "Lucas Bordeaux, Marco Cadoli, Toni Mancini",
    "title": "Generalizing Consistency and other Constraint Properties to Quantified\n  Constraints",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LO cs.AI",
    "license": null,
    "abstract": "  Quantified constraints and Quantified Boolean Formulae are typically much\nmore difficult to reason with than classical constraints, because quantifier\nalternation makes the usual notion of solution inappropriate. As a consequence,\nbasic properties of Constraint Satisfaction Problems (CSP), such as consistency\nor substitutability, are not completely understood in the quantified case.\nThese properties are important because they are the basis of most of the\nreasoning methods used to solve classical (existentially quantified)\nconstraints, and one would like to benefit from similar reasoning methods in\nthe resolution of quantified constraints. In this paper, we show that most of\nthe properties that are used by solvers for CSP can be generalized to\nquantified CSP. This requires a re-thinking of a number of basic concepts; in\nparticular, we propose a notion of outcome that generalizes the classical\nnotion of solution and on which all definitions are based. We propose a\nsystematic study of the relations which hold between these properties, as well\nas complexity results regarding the decision of these properties. Finally, and\nsince these problems are typically intractable, we generalize the approach used\nin CSP and propose weaker, easier to check notions based on locality, which\nallow to detect these properties incompletely but in polynomial time.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 24 May 2007 11:27:55 GMT"
      }
    ],
    "update_date": "2007-05-25",
    "authors_parsed": [
      [
        "Bordeaux",
        "Lucas",
        ""
      ],
      [
        "Cadoli",
        "Marco",
        ""
      ],
      [
        "Mancini",
        "Toni",
        ""
      ]
    ]
  },
  {
    "id": "0705.3766",
    "submitter": "Anton Eremeev",
    "authors": "Anton Eremeev",
    "title": "On complexity of optimized crossover for binary representations",
    "comments": "Dagstuhl Seminar 06061 \"Theory of Evolutionary Algorithms\", 2006",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI",
    "license": null,
    "abstract": "  We consider the computational complexity of producing the best possible\noffspring in a crossover, given two solutions of the parents. The crossover\noperators are studied on the class of Boolean linear programming problems,\nwhere the Boolean vector of variables is used as the solution representation.\nBy means of efficient reductions of the optimized gene transmitting crossover\nproblems (OGTC) we show the polynomial solvability of the OGTC for the maximum\nweight set packing problem, the minimum weight set partition problem and for\none of the versions of the simple plant location problem. We study a connection\nbetween the OGTC for linear Boolean programming problem and the maximum weight\nindependent set problem on 2-colorable hypergraph and prove the NP-hardness of\nseveral special cases of the OGTC problem in Boolean linear programming.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 25 May 2007 13:07:18 GMT"
      }
    ],
    "update_date": "2007-05-28",
    "authors_parsed": [
      [
        "Eremeev",
        "Anton",
        ""
      ]
    ]
  },
  {
    "id": "0705.4302",
    "submitter": "Jens Oehlschl\\\"agel",
    "authors": "Jens Oehlschl\\\"agel",
    "title": "Truecluster matching",
    "comments": "15 pages, 2 figures. Details the matching needed for \"Truecluster:\n  robust scalable clustering with model selection\" but can also be used in\n  different contexts",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 29 May 2007 21:52:17 GMT"
      }
    ],
    "update_date": "2007-05-31",
    "authors_parsed": [
      [
        "Oehlschlägel",
        "Jens",
        ""
      ]
    ]
  },
  {
    "id": "0705.4566",
    "submitter": "Bastian Wemmenhove",
    "authors": "Bastian Wemmenhove and Bert Kappen",
    "title": "Loop corrections for message passing algorithms in continuous variable\n  models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG",
    "license": null,
    "abstract": "  In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 31 May 2007 10:35:07 GMT"
      }
    ],
    "update_date": "2007-06-01",
    "authors_parsed": [
      [
        "Wemmenhove",
        "Bastian",
        ""
      ],
      [
        "Kappen",
        "Bert",
        ""
      ]
    ]
  },
  {
    "id": "0705.4584",
    "submitter": "Stefan Johansson",
    "authors": "Magnus Boman and Stefan J. Johansson",
    "title": "Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in\n  Massively Multiplayer Online Games",
    "comments": "Accepted for presentation at Digital Games Research Association\n  (DiGRA) conference in Tokyo in September 2007. All comments to the authors\n  (mail addresses are in the paper) are welcome",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.AI cs.MA",
    "license": null,
    "abstract": "  A virtual plague is a process in which a behavior-affecting property spreads\namong characters in a Massively Multiplayer Online Game (MMOG). The MMOG\nindividuals constitute a synthetic population, and the game can be seen as a\nform of interactive executable model for studying disease spread, albeit of a\nvery special kind. To a game developer maintaining an MMOG, recognizing,\nmonitoring, and ultimately controlling a virtual plague is important,\nregardless of how it was initiated. The prospect of using tools, methods and\ntheory from the field of epidemiology to do this seems natural and appealing.\nWe will address the feasibility of such a prospect, first by considering some\nbasic measures used in epidemiology, then by pointing out the differences\nbetween real world epidemics and virtual plagues. We also suggest directions\nfor MMOG developer control through epidemiological modeling. Our aim is\nunderstanding the properties of virtual plagues, rather than trying to\neliminate them or mitigate their effects, as would be in the case of real\ninfectious disease.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 31 May 2007 12:15:05 GMT"
      }
    ],
    "update_date": "2007-06-01",
    "authors_parsed": [
      [
        "Boman",
        "Magnus",
        ""
      ],
      [
        "Johansson",
        "Stefan J.",
        ""
      ]
    ]
  },
  {
    "id": "0706.0022",
    "submitter": "Marko Antonio Rodriguez",
    "authors": "Marko A. Rodriguez and Johan Bollen",
    "title": "Modeling Computations in a Semantic Network",
    "comments": "project website: http://neno.lanl.gov",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Semantic network research has seen a resurgence from its early history in the\ncognitive sciences with the inception of the Semantic Web initiative. The\nSemantic Web effort has brought forth an array of technologies that support the\nencoding, storage, and querying of the semantic network data structure at the\nworld stage. Currently, the popular conception of the Semantic Web is that of a\ndata modeling medium where real and conceptual entities are related in\nsemantically meaningful ways. However, new models have emerged that explicitly\nencode procedural information within the semantic network substrate. With these\nnew technologies, the Semantic Web has evolved from a data modeling medium to a\ncomputational medium. This article provides a classification of existing\ncomputational modeling efforts and the requirements of supporting technologies\nthat will aid in the further growth of this burgeoning domain.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 31 May 2007 21:56:25 GMT"
      }
    ],
    "update_date": "2021-08-23",
    "authors_parsed": [
      [
        "Rodriguez",
        "Marko A.",
        ""
      ],
      [
        "Bollen",
        "Johan",
        ""
      ]
    ]
  },
  {
    "id": "0706.0465",
    "submitter": "Donald Sofge",
    "authors": "D. A. Sofge",
    "title": "Virtual Sensor Based Fault Detection and Classification on a Plasma Etch\n  Reactor",
    "comments": "7 pages",
    "journal-ref": "D. Sofge, \"Virtual Sensor Based Fault Detection and Classification\n  on a Plasma Etch Reactor,\" The 2nd Joint Mexico-US Int'l. Workshop on Neural\n  Networks and Neurocontrol (poster), 1997",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CV",
    "license": null,
    "abstract": "  The SEMATECH sponsored J-88-E project teaming Texas Instruments with\nNeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam\n9600 aluminum plasma etch reactor, used in the process of semiconductor\nfabrication. Fault classification was accomplished by implementing a series of\nvirtual sensor models which used data from real sensors (Lam Station sensors,\nOptical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints\nand wafer state characteristics. Fault detection and classification were\nperformed by comparing predicted recipe and wafer state values with expected\nvalues. Models utilized include linear PLS, Polynomial PLS, and Neural Network\nPLS. Prediction of recipe setpoints based upon sensor data provides a\ncapability for cross-checking that the machine is maintaining the desired\nsetpoints. Wafer state characteristics such as Line Width Reduction and\nRemaining Oxide were estimated on-line using these same process sensors (Lam,\nOES, RFM). Wafer-to-wafer measurement of these characteristics in a production\nsetting (where typically this information may be only sparsely available, if at\nall, after batch processing runs with numerous wafers have been completed)\nwould provide important information to the operator that the process is or is\nnot producing wafers within acceptable bounds of product quality. Production\nyield is increased, and correspondingly per unit cost is reduced, by providing\nthe operator with the opportunity to adjust the process or machine before\netching more wafers.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Jun 2007 15:55:27 GMT"
      }
    ],
    "update_date": "2007-06-05",
    "authors_parsed": [
      [
        "Sofge",
        "D. A.",
        ""
      ]
    ]
  },
  {
    "id": "0706.0585",
    "submitter": "Zhendong Zhao",
    "authors": "Zhendong Zhao, Lei Yuan, Yuxuan Wang, Forrest Sheng Bao, Shunyi Zhang\n  Yanfei Sun",
    "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods",
    "comments": "8 pages, 12 figures, it was submitted to IEEE International\n  conference of Tools on Artificial Intelligence",
    "journal-ref": null,
    "doi": "10.1109/ICTAI.2007.99",
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": null,
    "abstract": "  In the process of training Support Vector Machines (SVMs) by decomposition\nmethods, working set selection is an important technique, and some exciting\nschemes were employed into this field. To improve working set selection, we\npropose a new model for working set selection in sequential minimal\noptimization (SMO) decomposition methods. In this model, it selects B as\nworking set without reselection. Some properties are given by simple proof, and\nexperiments demonstrate that the proposed method is in general faster than\nexisting methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 5 Jun 2007 05:55:07 GMT"
      }
    ],
    "update_date": "2016-11-15",
    "authors_parsed": [
      [
        "Zhao",
        "Zhendong",
        ""
      ],
      [
        "Yuan",
        "Lei",
        ""
      ],
      [
        "Wang",
        "Yuxuan",
        ""
      ],
      [
        "Bao",
        "Forrest Sheng",
        ""
      ],
      [
        "Sun",
        "Shunyi Zhang Yanfei",
        ""
      ]
    ]
  },
  {
    "id": "0706.1001",
    "submitter": "Krzysztof R. Apt",
    "authors": "Krzysztof R. Apt",
    "title": "Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets",
    "comments": "8 pages Proc. of the 11th Conference on Theoretical Aspects of\n  Rationality and Knowledge (TARK XI), 2007. To appear",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.GT cs.AI",
    "license": null,
    "abstract": "  We provide here an epistemic analysis of arbitrary strategic games based on\nthe possibility correspondences. Such an analysis calls for the use of\ntransfinite iterations of the corresponding operators. Our approach is based on\nTarski's Fixpoint Theorem and applies both to the notions of rationalizability\nand the iterated elimination of strictly dominated strategies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Jun 2007 12:57:21 GMT"
      }
    ],
    "update_date": "2007-06-08",
    "authors_parsed": [
      [
        "Apt",
        "Krzysztof R.",
        ""
      ]
    ]
  },
  {
    "id": "0706.1137",
    "submitter": "Thierry Poibeau",
    "authors": "Amanda Bouffier (LIPN), Thierry Poibeau (LIPN)",
    "title": "Automatically Restructuring Practice Guidelines using the GEM DTD",
    "comments": null,
    "journal-ref": "Proceedings of Biomedical Natural Language Processing (BioNLP)\n  (2007) -",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present a system developed for this task. We show that it\nyields good performance when applied to the analysis of French practice\nguidelines.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 8 Jun 2007 15:39:49 GMT"
      }
    ],
    "update_date": "2007-06-11",
    "authors_parsed": [
      [
        "Bouffier",
        "Amanda",
        "",
        "LIPN"
      ],
      [
        "Poibeau",
        "Thierry",
        "",
        "LIPN"
      ]
    ]
  },
  {
    "id": "0706.1290",
    "submitter": "Sylviane Schwer",
    "authors": "Sylviane R. Schwer (LIPN)",
    "title": "Temporal Reasoning without Transitive Tables",
    "comments": "rapport interne",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Representing and reasoning about qualitative temporal information is an\nessential part of many artificial intelligence tasks. Lots of models have been\nproposed in the litterature for representing such temporal information. All\nderive from a point-based or an interval-based framework. One fundamental\nreasoning task that arises in applications of these frameworks is given by the\nfollowing scheme: given possibly indefinite and incomplete knowledge of the\nbinary relationships between some temporal objects, find the consistent\nscenarii between all these objects. All these models require transitive tables\n-- or similarly inference rules-- for solving such tasks. We have defined an\nalternative model, S-languages - to represent qualitative temporal information,\nbased on the only two relations of \\emph{precedence} and \\emph{simultaneity}.\nIn this paper, we show how this model enables to avoid transitive tables or\ninference rules to handle this kind of problem.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 9 Jun 2007 06:57:05 GMT"
      }
    ],
    "update_date": "2007-06-12",
    "authors_parsed": [
      [
        "Schwer",
        "Sylviane R.",
        "",
        "LIPN"
      ]
    ]
  },
  {
    "id": "0706.3639",
    "submitter": "Marcus Hutter",
    "authors": "Shane Legg and Marcus Hutter",
    "title": "A Collection of Definitions of Intelligence",
    "comments": "12 LaTeX pages",
    "journal-ref": "Frontiers in Artificial Intelligence and Applications, Vol.157\n  (2007) 17-24",
    "doi": null,
    "report-no": "IDSIA-07-07",
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper is a survey of a large number of informal definitions of\n``intelligence'' that the authors have collected over the years. Naturally,\ncompiling a complete list would be impossible as many definitions of\nintelligence are buried deep inside articles and books. Nevertheless, the\n70-odd definitions presented here are, to the authors' knowledge, the largest\nand most well referenced collection there is.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 25 Jun 2007 13:40:56 GMT"
      }
    ],
    "update_date": "2007-06-26",
    "authors_parsed": [
      [
        "Legg",
        "Shane",
        ""
      ],
      [
        "Hutter",
        "Marcus",
        ""
      ]
    ]
  },
  {
    "id": "0706.4323",
    "submitter": "Khalil Djelloul",
    "authors": "Khalil Djelloul, Thi-bich-hanh Dao and Thom Fruehwirth",
    "title": "Theory of Finite or Infinite Trees Revisited",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LO cs.AI",
    "license": null,
    "abstract": "  We present in this paper a first-order axiomatization of an extended theory\n$T$ of finite or infinite trees, built on a signature containing an infinite\nset of function symbols and a relation $\\fini(t)$ which enables to distinguish\nbetween finite or infinite trees. We show that $T$ has at least one model and\nprove its completeness by giving not only a decision procedure, but a full\nfirst-order constraint solver which gives clear and explicit solutions for any\nfirst-order constraint satisfaction problem in $T$. The solver is given in the\nform of 16 rewriting rules which transform any first-order constraint $\\phi$\ninto an equivalent disjunction $\\phi$ of simple formulas such that $\\phi$ is\neither the formula $\\true$ or the formula $\\false$ or a formula having at least\none free variable, being equivalent neither to $\\true$ nor to $\\false$ and\nwhere the solutions of the free variables are expressed in a clear and explicit\nway. The correctness of our rules implies the completeness of $T$. We also\ndescribe an implementation of our algorithm in CHR (Constraint Handling Rules)\nand compare the performance with an implementation in C++ and that of a recent\ndecision procedure for decomposable theories.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 28 Jun 2007 21:18:19 GMT"
      }
    ],
    "update_date": "2007-07-02",
    "authors_parsed": [
      [
        "Djelloul",
        "Khalil",
        ""
      ],
      [
        "Dao",
        "Thi-bich-hanh",
        ""
      ],
      [
        "Fruehwirth",
        "Thom",
        ""
      ]
    ]
  },
  {
    "id": "0706.4375",
    "submitter": "Thierry Hamon",
    "authors": "Thierry Hamon (LIPN), Adeline Nazarenko (LIPN), Thierry Poibeau\n  (LIPN), Sophie Aubin (LIPN), Julien Derivi\\`ere (LIPN)",
    "title": "A Robust Linguistic Platform for Efficient and Domain specific Web\n  Content Analysis",
    "comments": null,
    "journal-ref": "Proceedings of RIAO 2007 (30/05/2007)",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Web semantic access in specific domains calls for specialized search engines\nwith enhanced semantic querying and indexing capacities, which pertain both to\ninformation retrieval (IR) and to information extraction (IE). A rich\nlinguistic analysis is required either to identify the relevant semantic units\nto index and weight them according to linguistic specific statistical\ndistribution, or as the basis of an information extraction process. Recent\ndevelopments make Natural Language Processing (NLP) techniques reliable enough\nto process large collections of documents and to enrich them with semantic\nannotations. This paper focuses on the design and the development of a text\nprocessing platform, Ogmios, which has been developed in the ALVIS project. The\nOgmios platform exploits existing NLP modules and resources, which may be tuned\nto specific domains and produces linguistically annotated documents. We show\nhow the three constraints of genericity, domain semantic awareness and\nperformance can be handled all together.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 29 Jun 2007 08:58:02 GMT"
      }
    ],
    "update_date": "2007-07-02",
    "authors_parsed": [
      [
        "Hamon",
        "Thierry",
        "",
        "LIPN"
      ],
      [
        "Nazarenko",
        "Adeline",
        "",
        "LIPN"
      ],
      [
        "Poibeau",
        "Thierry",
        "",
        "LIPN"
      ],
      [
        "Aubin",
        "Sophie",
        "",
        "LIPN"
      ],
      [
        "Derivière",
        "Julien",
        "",
        "LIPN"
      ]
    ]
  },
  {
    "id": "0707.0498",
    "submitter": "Roy Murphy Dr",
    "authors": "Roy E. Murphy",
    "title": "The Role of Time in the Creation of Knowledge",
    "comments": "Adaptive Processes",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.IT math.IT",
    "license": null,
    "abstract": "  This paper I assume that in humans the creation of knowledge depends on a\ndiscrete time, or stage, sequential decision-making process subjected to a\nstochastic, information transmitting environment. For each time-stage, this\nenvironment randomly transmits Shannon type information-packets to the\ndecision-maker, who examines each of them for relevancy and then determines his\noptimal choices. Using this set of relevant information-packets, the\ndecision-maker adapts, over time, to the stochastic nature of his environment,\nand optimizes the subjective expected rate-of-growth of knowledge. The\ndecision-maker's optimal actions, lead to a decision function that involves,\nover time, his view of the subjective entropy of the environmental process and\nother important parameters at each time-stage of the process. Using this model\nof human behavior, one could create psychometric experiments using computer\nsimulation and real decision-makers, to play programmed games to measure the\nresulting human performance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 Jul 2007 20:43:43 GMT"
      }
    ],
    "update_date": "2007-07-13",
    "authors_parsed": [
      [
        "Murphy",
        "Roy E.",
        ""
      ]
    ]
  },
  {
    "id": "0707.0701",
    "submitter": "Alexandre d'Aspremont",
    "authors": "Ronny Luss, Alexandre d'Aspremont",
    "title": "Clustering and Feature Selection using Sparse Principal Component\n  Analysis",
    "comments": "More experiments",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG cs.MS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 4 Jul 2007 21:53:11 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 8 Oct 2008 18:41:53 GMT"
      }
    ],
    "update_date": "2008-10-08",
    "authors_parsed": [
      [
        "Luss",
        "Ronny",
        ""
      ],
      [
        "d'Aspremont",
        "Alexandre",
        ""
      ]
    ]
  },
  {
    "id": "0707.0704",
    "submitter": "Alexandre d'Aspremont",
    "authors": "Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont",
    "title": "Model Selection Through Sparse Maximum Likelihood Estimation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG",
    "license": null,
    "abstract": "  We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright & Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 4 Jul 2007 22:13:42 GMT"
      }
    ],
    "update_date": "2007-07-06",
    "authors_parsed": [
      [
        "Banerjee",
        "Onureena",
        ""
      ],
      [
        "Ghaoui",
        "Laurent El",
        ""
      ],
      [
        "d'Aspremont",
        "Alexandre",
        ""
      ]
    ]
  },
  {
    "id": "0707.0705",
    "submitter": "Alexandre d'Aspremont",
    "authors": "Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui",
    "title": "Optimal Solutions for Sparse Principal Component Analysis",
    "comments": "Revised journal version. More efficient optimality conditions and new\n  examples in subset selection and sparse recovery. Original version is in ICML\n  proceedings",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG",
    "license": null,
    "abstract": "  Given a sample covariance matrix, we examine the problem of maximizing the\nvariance explained by a linear combination of the input variables while\nconstraining the number of nonzero coefficients in this combination. This is\nknown as sparse principal component analysis and has a wide array of\napplications in machine learning and engineering. We formulate a new\nsemidefinite relaxation to this problem and derive a greedy algorithm that\ncomputes a full set of good solutions for all target numbers of non zero\ncoefficients, with total complexity O(n^3), where n is the number of variables.\nWe then use the same relaxation to derive sufficient conditions for global\noptimality of a solution, which can be tested in O(n^3) per pattern. We discuss\napplications in subset selection and sparse recovery and show on artificial\nexamples and biological data that our algorithm does provide globally optimal\nsolutions in many cases.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 4 Jul 2007 22:28:28 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 3 Aug 2007 22:30:22 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 24 Aug 2007 00:49:31 GMT"
      },
      {
        "version": "v4",
        "created": "Fri, 9 Nov 2007 17:27:11 GMT"
      }
    ],
    "update_date": "2011-11-10",
    "authors_parsed": [
      [
        "d'Aspremont",
        "Alexandre",
        ""
      ],
      [
        "Bach",
        "Francis",
        ""
      ],
      [
        "Ghaoui",
        "Laurent El",
        ""
      ]
    ]
  },
  {
    "id": "0707.0808",
    "submitter": "Patrick C. McGuire",
    "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
    "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
    "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
    "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
    "doi": "10.1017/S1473550407003862",
    "report-no": null,
    "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
    "license": null,
    "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 5 Jul 2007 15:19:37 GMT"
      }
    ],
    "update_date": "2010-01-08",
    "authors_parsed": [
      [
        "Bartolo",
        "Alexandra",
        ""
      ],
      [
        "McGuire",
        "Patrick C.",
        ""
      ],
      [
        "Camilleri",
        "Kenneth P.",
        ""
      ],
      [
        "Spiteri",
        "Christopher",
        ""
      ],
      [
        "Borg",
        "Jonathan C.",
        ""
      ],
      [
        "Farrugia",
        "Philip J.",
        ""
      ],
      [
        "Ormo",
        "Jens",
        ""
      ],
      [
        "Gomez-Elvira",
        "Javier",
        ""
      ],
      [
        "Rodriguez-Manfredi",
        "Jose Antonio",
        ""
      ],
      [
        "Diaz-Martinez",
        "Enrique",
        ""
      ],
      [
        "Ritter",
        "Helge",
        ""
      ],
      [
        "Haschke",
        "Robert",
        ""
      ],
      [
        "Oesker",
        "Markus",
        ""
      ],
      [
        "Ontrup",
        "Joerg",
        ""
      ]
    ]
  },
  {
    "id": "0707.1452",
    "submitter": "Xavier Polanco",
    "authors": "Xavier Polanco (INIST)",
    "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported\n  Communication within a Virtual Community",
    "comments": null,
    "journal-ref": "Advances in Knowledge Organization (2002) 364-371",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG",
    "license": null,
    "abstract": "  The proposal is to use clusters, graphs and networks as models in order to\nanalyse the Web structure. Clusters, graphs and networks provide knowledge\nrepresentation and organization. Clusters were generated by co-site analysis.\nThe sample is a set of academic Web sites from the countries belonging to the\nEuropean Union. These clusters are here revisited from the point of view of\ngraph theory and social network analysis. This is a quantitative and structural\nanalysis. In fact, the Internet is a computer network that connects people and\norganizations. Thus we may consider it to be a social network. The set of Web\nacademic sites represents an empirical social network, and is viewed as a\nvirtual community. The network structural properties are here analysed applying\ntogether cluster analysis, graph theory and social network analysis.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 10 Jul 2007 13:47:32 GMT"
      }
    ],
    "update_date": "2007-07-11",
    "authors_parsed": [
      [
        "Polanco",
        "Xavier",
        "",
        "INIST"
      ]
    ]
  },
  {
    "id": "0707.2506",
    "submitter": "Alain Dutech",
    "authors": "Raghav Aras (INRIA Lorraine - LORIA), Alain Dutech (INRIA Lorraine -\n  LORIA), Fran\\c{c}ois Charpillet (INRIA Lorraine - LORIA)",
    "title": "Mixed Integer Linear Programming For Exact Finite-Horizon Planning In\n  Decentralized Pomdps",
    "comments": null,
    "journal-ref": "Dans The International Conference on Automated Planning and\n  Scheduling (2007)",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  We consider the problem of finding an n-agent joint-policy for the optimal\nfinite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem\nof very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new\nmathematical programming approach for the problem. Our approach is based on two\nideas: First, we represent each agent's policy in the sequence-form and not in\nthe tree-form, thereby obtaining a very compact representation of the set of\njoint-policies. Second, using this compact representation, we solve this\nproblem as an instance of combinatorial optimization for which we formulate a\nmixed integer linear program (MILP). The optimal solution of the MILP directly\nyields an optimal joint-policy for the Dec-Pomdp. Computational experience\nshows that formulating and solving the MILP requires significantly less time to\nsolve benchmark Dec-Pomdp problems than existing algorithms. For example, the\nmulti-agent tiger problem for horizon 4 is solved in 72 secs with the MILP\nwhereas existing algorithms require several hours to solve it.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Jul 2007 12:49:30 GMT"
      }
    ],
    "update_date": "2016-03-28",
    "authors_parsed": [
      [
        "Aras",
        "Raghav",
        "",
        "INRIA Lorraine - LORIA"
      ],
      [
        "Dutech",
        "Alain",
        "",
        "INRIA Lorraine -\n  LORIA"
      ],
      [
        "Charpillet",
        "François",
        "",
        "INRIA Lorraine - LORIA"
      ]
    ]
  },
  {
    "id": "0707.3030",
    "submitter": "Matthias Brust R.",
    "authors": "Gregoire Danoy, Pascal Bouvry, Matthias R. Brust, Enrique Alba",
    "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
    "comments": "1 page, 1 figure",
    "journal-ref": "Genetic and Evolutionary Computation Conference (GECCO 2007), ISBN\n  978-1-59593-697-4",
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI cs.NI",
    "license": null,
    "abstract": "  This work aims at optimizing injection networks, which consist in adding a\nset of long-range links (called bypass links) in mobile multi-hop ad hoc\nnetworks so as to improve connectivity and overcome network partitioning. To\nthis end, we rely on small-world network properties, that comprise a high\nclustering coefficient and a low characteristic path length. We investigate the\nuse of two genetic algorithms (generational and steady-state) to optimize three\ninstances of this topology control problem and present results that show\ninitial evidence of their capacity to solve it.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 20 Jul 2007 10:07:27 GMT"
      }
    ],
    "update_date": "2007-07-23",
    "authors_parsed": [
      [
        "Danoy",
        "Gregoire",
        ""
      ],
      [
        "Bouvry",
        "Pascal",
        ""
      ],
      [
        "Brust",
        "Matthias R.",
        ""
      ],
      [
        "Alba",
        "Enrique",
        ""
      ]
    ]
  },
  {
    "id": "0707.3205",
    "submitter": "Andrew Schumann",
    "authors": "Andrew Schumann, Florentin Smarandache",
    "title": "Neutrality and Many-Valued Logics",
    "comments": "119 pages",
    "journal-ref": "A. Schumann, F. Smarandache, Neutrality and Many-Valued Logics.\n  American Research Press, 2007",
    "doi": null,
    "report-no": null,
    "categories": "cs.LO cs.AI",
    "license": null,
    "abstract": "  In this book, we consider various many-valued logics: standard, linear,\nhyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We\nsurvey also results which show the tree different proof-theoretic frameworks\nfor many-valued logics, e.g. frameworks of the following deductive calculi:\nHilbert's style, sequent, and hypersequent. We present a general way that\nallows to construct systematically analytic calculi for a large family of\nnon-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and\np-adic valued logics characterized by a special format of semantics with an\nappropriate rejection of Archimedes' axiom. These logics are built as different\nextensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's,\nProduct, and Post's logics). The informal sense of Archimedes' axiom is that\nanything can be measured by a ruler. Also logical multiple-validity without\nArchimedes' axiom consists in that the set of truth values is infinite and it\nis not well-founded and well-ordered. On the base of non-Archimedean valued\nlogics, we construct non-Archimedean valued interval neutrosophic logic INL by\nwhich we can describe neutrality phenomena.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 21 Jul 2007 10:35:37 GMT"
      }
    ],
    "update_date": "2007-07-24",
    "authors_parsed": [
      [
        "Schumann",
        "Andrew",
        ""
      ],
      [
        "Smarandache",
        "Florentin",
        ""
      ]
    ]
  },
  {
    "id": "0707.3457",
    "submitter": "Chenguang Lu",
    "authors": "Chenguang Lu",
    "title": "A Generalized Information Formula as the Bridge between Shannon and\n  Popper",
    "comments": "8 pages, 5 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.IT cs.AI math.IT",
    "license": null,
    "abstract": "  A generalized information formula related to logical probability and fuzzy\nset is deduced from the classical information formula. The new information\nmeasure accords with to Popper's criterion for knowledge evolution very much.\nIn comparison with square error criterion, the information criterion does not\nonly reflect error of a proposition, but also reflects the particularity of the\nevent described by the proposition. It gives a proposition with less logical\nprobability higher evaluation. The paper introduces how to select a prediction\nor sentence from many for forecasts and language translations according to the\ngeneralized information criterion. It also introduces the rate fidelity theory,\nwhich comes from the improvement of the rate distortion theory in the classical\ninformation theory by replacing distortion (i.e. average error) criterion with\nthe generalized mutual information criterion, for data compression and\ncommunication efficiency. Some interesting conclusions are obtained from the\nrate-fidelity function in relation to image communication. It also discusses\nhow to improve Popper's theory.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 24 Jul 2007 00:04:32 GMT"
      }
    ],
    "update_date": "2007-07-25",
    "authors_parsed": [
      [
        "Lu",
        "Chenguang",
        ""
      ]
    ]
  },
  {
    "id": "0707.3559",
    "submitter": "Wilson Wong",
    "authors": "Wilson Wong",
    "title": "Practical Approach to Knowledge-based Question Answering with Natural\n  Language Understanding and Advanced Reasoning",
    "comments": "Master of Science thesis, National Technical University College of\n  Malaysia, 2005",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.HC cs.IR",
    "license": null,
    "abstract": "  This research hypothesized that a practical approach in the form of a\nsolution framework known as Natural Language Understanding and Reasoning for\nIntelligence (NaLURI), which combines full-discourse natural language\nunderstanding, powerful representation formalism capable of exploiting\nontological information and reasoning approach with advanced features, will\nsolve the following problems without compromising practicality factors: 1)\nrestriction on the nature of question and response, and 2) limitation to scale\nacross domains and to real-life natural language text.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 24 Jul 2007 14:30:27 GMT"
      }
    ],
    "update_date": "2007-07-25",
    "authors_parsed": [
      [
        "Wong",
        "Wilson",
        ""
      ]
    ]
  },
  {
    "id": "0707.3781",
    "submitter": "Paolo Liberatore",
    "authors": "Paolo Liberatore",
    "title": "Bijective Faithful Translations among Default Logics",
    "comments": "Removed one useless section",
    "journal-ref": null,
    "doi": "10.1093/logcom/ext073",
    "report-no": null,
    "categories": "cs.AI cs.LO",
    "license": null,
    "abstract": "  In this article, we study translations between variants of defaults logics\nsuch that the extensions of the theories that are the input and the output of\nthe translation are in a bijective correspondence. We assume that a translation\ncan introduce new variables and that the result of translating a theory can\neither be produced in time polynomial in the size of the theory or its output\nis polynomial in that size; we however restrict to the case in which the\noriginal theory has extensions. This study fills a gap between two previous\npieces of work, one studying bijective translations among restrictions of\ndefault logics, and the other one studying non-bijective translations between\ndefault logics variants.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Jul 2007 17:03:57 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 30 Jul 2007 13:46:43 GMT"
      }
    ],
    "update_date": "2021-04-12",
    "authors_parsed": [
      [
        "Liberatore",
        "Paolo",
        ""
      ]
    ]
  },
  {
    "id": "0707.3972",
    "submitter": "Ted Pedersen",
    "authors": "Ted Pedersen",
    "title": "Learning Probabilistic Models of Word Sense Disambiguation",
    "comments": "195 pages",
    "journal-ref": "PhD dissertation, May 1998, Department of Computer Science and\n  Engineering, Southern Methodist University",
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": null,
    "abstract": "  This dissertation presents several new methods of supervised and unsupervised\nlearning of word sense disambiguation models. The supervised methods focus on\nperforming model searches through a space of probabilistic models, and the\nunsupervised methods rely on the use of Gibbs Sampling and the Expectation\nMaximization (EM) algorithm. In both the supervised and unsupervised case, the\nNaive Bayesian model is found to perform well. An explanation for this success\nis presented in terms of learning rates and bias-variance decompositions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 26 Jul 2007 17:02:40 GMT"
      }
    ],
    "update_date": "2009-09-29",
    "authors_parsed": [
      [
        "Pedersen",
        "Ted",
        ""
      ]
    ]
  },
  {
    "id": "0707.4289",
    "submitter": "Sheng Bao",
    "authors": "Stephen Gang Wu, Forrest Sheng Bao, Eric You Xu, Yu-Xuan Wang, Yi-Fan\n  Chang and Qiao-Liang Xiang",
    "title": "A Leaf Recognition Algorithm for Plant Classification Using\n  Probabilistic Neural Network",
    "comments": "6 pages, 3 figures, 2 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  In this paper, we employ Probabilistic Neural Network (PNN) with image and\ndata processing techniques to implement a general purpose automated leaf\nrecognition algorithm. 12 leaf features are extracted and orthogonalized into 5\nprincipal variables which consist the input vector of the PNN. The PNN is\ntrained by 1800 leaves to classify 32 kinds of plants with an accuracy greater\nthan 90%. Compared with other approaches, our algorithm is an accurate\nartificial intelligence approach which is fast in execution and easy in\nimplementation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 29 Jul 2007 12:31:40 GMT"
      }
    ],
    "update_date": "2007-07-31",
    "authors_parsed": [
      [
        "Wu",
        "Stephen Gang",
        ""
      ],
      [
        "Bao",
        "Forrest Sheng",
        ""
      ],
      [
        "Xu",
        "Eric You",
        ""
      ],
      [
        "Wang",
        "Yu-Xuan",
        ""
      ],
      [
        "Chang",
        "Yi-Fan",
        ""
      ],
      [
        "Xiang",
        "Qiao-Liang",
        ""
      ]
    ]
  },
  {
    "id": "0708.0505",
    "submitter": "Luca Di Gaspero PhD",
    "authors": "Luca Di Gaspero, Andrea Roli",
    "title": "A preliminary analysis on metaheuristics methods applied to the\n  Haplotype Inference Problem",
    "comments": "22 pages, 4 figures Technical Report: DEIS - Alma Mater Studiorum,\n  University of Bologna no. DEIS-LIA-006-07",
    "journal-ref": null,
    "doi": null,
    "report-no": "DEIS-LIA-006-07",
    "categories": "cs.AI cs.CE cs.DM q-bio.QM",
    "license": null,
    "abstract": "  Haplotype Inference is a challenging problem in bioinformatics that consists\nin inferring the basic genetic constitution of diploid organisms on the basis\nof their genotype. This information allows researchers to perform association\nstudies for the genetic variants involved in diseases and the individual\nresponses to therapeutic agents.\n  A notable approach to the problem is to encode it as a combinatorial problem\n(under certain hypotheses, such as the pure parsimony criterion) and to solve\nit using off-the-shelf combinatorial optimization techniques. The main methods\napplied to Haplotype Inference are either simple greedy heuristic or exact\nmethods (Integer Linear Programming, Semidefinite Programming, SAT encoding)\nthat, at present, are adequate only for moderate size instances.\n  We believe that metaheuristic and hybrid approaches could provide a better\nscalability. Moreover, metaheuristics can be very easily combined with problem\nspecific heuristics and they can also be integrated with tree-based search\ntechniques, thus providing a promising framework for hybrid systems in which a\ngood trade-off between effectiveness and efficiency can be reached.\n  In this paper we illustrate a feasibility study of the approach and discuss\nsome relevant design issues, such as modeling and design of approximate solvers\nthat combine constructive heuristics, local search-based improvement strategies\nand learning mechanisms. Besides the relevance of the Haplotype Inference\nproblem itself, this preliminary analysis is also an interesting case study\nbecause the formulation of the problem poses some challenges in modeling and\nhybrid metaheuristic solver design that can be generalized to other problems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 3 Aug 2007 12:49:21 GMT"
      }
    ],
    "update_date": "2007-08-06",
    "authors_parsed": [
      [
        "Di Gaspero",
        "Luca",
        ""
      ],
      [
        "Roli",
        "Andrea",
        ""
      ]
    ]
  },
  {
    "id": "0708.0927",
    "submitter": "Emanuel Diamant",
    "authors": "Emanuel Diamant",
    "title": "Modeling Visual Information Processing in Brain: A Computer Vision Point\n  of View and Approach",
    "comments": "That is a journal version of a paper that in 2007 has been submitted\n  to 15 computer vision conferences and was discarded by 11 of them",
    "journal-ref": "Signal Processing: Image Communication, vol. 22, issue 6, pp.\n  583-590, July 2007",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CV",
    "license": null,
    "abstract": "  We live in the Information Age, and information has become a critically\nimportant component of our life. The success of the Internet made huge amounts\nof it easily available and accessible to everyone. To keep the flow of this\ninformation manageable, means for its faultless circulation and effective\nhandling have become urgently required. Considerable research efforts are\ndedicated today to address this necessity, but they are seriously hampered by\nthe lack of a common agreement about \"What is information?\" In particular, what\nis \"visual information\" - human's primary input from the surrounding world. The\nproblem is further aggravated by a long-lasting stance borrowed from the\nbiological vision research that assumes human-like information processing as an\nenigmatic mix of perceptual and cognitive vision faculties. I am trying to find\na remedy for this bizarre situation. Relying on a new definition of\n\"information\", which can be derived from Kolmogorov's compexity theory and\nChaitin's notion of algorithmic information, I propose a unifying framework for\nvisual information processing, which explicitly accounts for the perceptual and\ncognitive image processing peculiarities. I believe that this framework will be\nuseful to overcome the difficulties that are impeding our attempts to develop\nthe right model of human-like intelligent image processing.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 7 Aug 2007 11:16:15 GMT"
      }
    ],
    "update_date": "2007-08-08",
    "authors_parsed": [
      [
        "Diamant",
        "Emanuel",
        ""
      ]
    ]
  },
  {
    "id": "0708.1150",
    "submitter": "Marko Antonio Rodriguez",
    "authors": "Marko A. Rodriguez, Johah Bollen, Herbert Van de Sompel",
    "title": "A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts\n  and their Usage",
    "comments": null,
    "journal-ref": "Proceedings of the IEEE/ACM Joint Conference on Digital Libraries\n  (JCDL'07), pp. 278-287, 2007",
    "doi": "10.1145/1255175.1255229",
    "report-no": null,
    "categories": "cs.DL cs.AI",
    "license": null,
    "abstract": "  The large-scale analysis of scholarly artifact usage is constrained primarily\nby current practices in usage data archiving, privacy issues concerned with the\ndissemination of usage data, and the lack of a practical ontology for modeling\nthe usage domain. As a remedy to the third constraint, this article presents a\nscholarly ontology that was engineered to represent those classes for which\nlarge-scale bibliographic and usage data exists, supports usage research, and\nwhose instantiation is scalable to the order of 50 million articles along with\ntheir associated artifacts (e.g. authors and journals) and an accompanying 1\nbillion usage events. The real world instantiation of the presented abstract\nontology is a semantic network model of the scholarly community which lends the\nscholarly process to statistical analysis and computational support. We present\nthe ontology, discuss its instantiation, and provide some example inference\nrules for calculating various scholarly artifact metrics.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Aug 2007 17:06:55 GMT"
      }
    ],
    "update_date": "2007-08-09",
    "authors_parsed": [
      [
        "Rodriguez",
        "Marko A.",
        ""
      ],
      [
        "Bollen",
        "Johah",
        ""
      ],
      [
        "Van de Sompel",
        "Herbert",
        ""
      ]
    ]
  },
  {
    "id": "0708.1527",
    "submitter": "Stasinos Konstantopoulos",
    "authors": "Stasinos Konstantopoulos",
    "title": "A Data-Parallel Version of Aleph",
    "comments": "Proceedings of Parallel and Distributed Computing for Machine\n  Learning workshop, held in conjunction with the 14th European Conference on\n  Machine Learning. Cavtat, Croatia, 2003",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.DC",
    "license": null,
    "abstract": "  This is to present work on modifying the Aleph ILP system so that it\nevaluates the hypothesised clauses in parallel by distributing the data-set\namong the nodes of a parallel or distributed machine. The paper briefly\ndiscusses MPI, the interface used to access message- passing libraries for\nparallel computers and clusters. It then proceeds to describe an extension of\nYAP Prolog with an MPI interface and an implementation of data-parallel clause\nevaluation for Aleph through this interface. The paper concludes by testing the\ndata-parallel Aleph on artificially constructed data-sets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 10 Aug 2007 23:32:16 GMT"
      }
    ],
    "update_date": "2007-08-14",
    "authors_parsed": [
      [
        "Konstantopoulos",
        "Stasinos",
        ""
      ]
    ]
  },
  {
    "id": "0708.1964",
    "submitter": "Mihai Oltean",
    "authors": "Mihai Oltean, Oana Muntean",
    "title": "Solving the subset-sum problem with a light-based device",
    "comments": "14 pages, 6 figures, Natural Computing, 2007",
    "journal-ref": "Natural Computing, Springer-Verlag, Vol 8, Issue 2, pp. 321-331,\n  2009",
    "doi": "10.1007/s11047-007-9059-3",
    "report-no": null,
    "categories": "cs.AR cs.AI cs.DC",
    "license": null,
    "abstract": "  We propose a special computational device which uses light rays for solving\nthe subset-sum problem. The device has a graph-like representation and the\nlight is traversing it by following the routes given by the connections between\nnodes. The nodes are connected by arcs in a special way which lets us to\ngenerate all possible subsets of the given set. To each arc we assign either a\nnumber from the given set or a predefined constant. When the light is passing\nthrough an arc it is delayed by the amount of time indicated by the number\nplaced in that arc. At the destination node we will check if there is a ray\nwhose total delay is equal to the target value of the subset sum problem (plus\nsome constants).\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 14 Aug 2007 21:46:32 GMT"
      }
    ],
    "update_date": "2015-09-11",
    "authors_parsed": [
      [
        "Oltean",
        "Mihai",
        ""
      ],
      [
        "Muntean",
        "Oana",
        ""
      ]
    ]
  },
  {
    "id": "0708.2303",
    "submitter": "W Saba",
    "authors": "Walid S. Saba",
    "title": "Compositional Semantics Grounded in Commonsense Metaphysics",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CL",
    "license": null,
    "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology\nthat reflects our commonsense view of the world and the way we talk about it in\nordinary language. Assuming the existence of such a structure, we show that the\nsemantics of various natural language phenomena may become nearly trivial.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 17 Aug 2007 01:15:11 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 31 Aug 2007 17:48:14 GMT"
      }
    ],
    "update_date": "2009-09-29",
    "authors_parsed": [
      [
        "Saba",
        "Walid S.",
        ""
      ]
    ]
  },
  {
    "id": "0708.2432",
    "submitter": "Oliver Knill",
    "authors": "Oliver Knill and Jose Ramirez-Herran",
    "title": "A structure from motion inequality",
    "comments": "15 pages, 22 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": null,
    "abstract": "  We state an elementary inequality for the structure from motion problem for m\ncameras and n points. This structure from motion inequality relates space\ndimension, camera parameter dimension, the number of cameras and number points\nand global symmetry properties and provides a rigorous criterion for which\nreconstruction is not possible with probability 1. Mathematically the\ninequality is based on Frobenius theorem which is a geometric incarnation of\nthe fundamental theorem of linear algebra. The paper also provides a general\nmathematical formalism for the structure from motion problem. It includes the\nsituation the points can move while the camera takes the pictures.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 18 Aug 2007 14:36:28 GMT"
      }
    ],
    "update_date": "2007-08-21",
    "authors_parsed": [
      [
        "Knill",
        "Oliver",
        ""
      ],
      [
        "Ramirez-Herran",
        "Jose",
        ""
      ]
    ]
  },
  {
    "id": "0708.2438",
    "submitter": "Oliver Knill",
    "authors": "Oliver Knill and Jose Ramirez-Herran",
    "title": "On Ullman's theorem in computer vision",
    "comments": "16 pages, 13 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": null,
    "abstract": "  Both in the plane and in space, we invert the nonlinear Ullman transformation\nfor 3 points and 3 orthographic cameras. While Ullman's theorem assures a\nunique reconstruction modulo a reflection for 3 cameras and 4 points, we find a\nlocally unique reconstruction for 3 cameras and 3 points. Explicit\nreconstruction formulas allow to decide whether picture data of three cameras\nseeing three points can be realized as a point-camera configuration.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 17 Aug 2007 21:36:08 GMT"
      }
    ],
    "update_date": "2007-08-21",
    "authors_parsed": [
      [
        "Knill",
        "Oliver",
        ""
      ],
      [
        "Ramirez-Herran",
        "Jose",
        ""
      ]
    ]
  },
  {
    "id": "0708.2442",
    "submitter": "Oliver Knill",
    "authors": "Oliver Knill and Jose Ramirez-Herran",
    "title": "Space and camera path reconstruction for omni-directional vision",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": null,
    "abstract": "  In this paper, we address the inverse problem of reconstructing a scene as\nwell as the camera motion from the image sequence taken by an omni-directional\ncamera. Our structure from motion results give sharp conditions under which the\nreconstruction is unique. For example, if there are three points in general\nposition and three omni-directional cameras in general position, a unique\nreconstruction is possible up to a similarity. We then look at the\nreconstruction problem with m cameras and n points, where n and m can be large\nand the over-determined system is solved by least square methods. The\nreconstruction is robust and generalizes to the case of a dynamic environment\nwhere landmarks can move during the movie capture. Possible applications of the\nresult are computer assisted scene reconstruction, 3D scanning, autonomous\nrobot navigation, medical tomography and city reconstructions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 17 Aug 2007 21:53:41 GMT"
      }
    ],
    "update_date": "2007-08-21",
    "authors_parsed": [
      [
        "Knill",
        "Oliver",
        ""
      ],
      [
        "Ramirez-Herran",
        "Jose",
        ""
      ]
    ]
  },
  {
    "id": "0708.4170",
    "submitter": "Paolo Liberatore",
    "authors": "Paolo Liberatore",
    "title": "Raising a Hardness Result",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CC cs.LO",
    "license": null,
    "abstract": "  This article presents a technique for proving problems hard for classes of\nthe polynomial hierarchy or for PSPACE. The rationale of this technique is that\nsome problem restrictions are able to simulate existential or universal\nquantifiers. If this is the case, reductions from Quantified Boolean Formulae\n(QBF) to these restrictions can be transformed into reductions from QBFs having\none more quantifier in the front. This means that a proof of hardness of a\nproblem at level n in the polynomial hierarchy can be split into n separate\nproofs, which may be simpler than a proof directly showing a reduction from a\nclass of QBFs to the considered problem.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 30 Aug 2007 14:42:50 GMT"
      }
    ],
    "update_date": "2007-08-31",
    "authors_parsed": [
      [
        "Liberatore",
        "Paolo",
        ""
      ]
    ]
  },
  {
    "id": "0708.4311",
    "submitter": "Juergen Schmidhuber",
    "authors": "Juergen Schmidhuber",
    "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25\n  Years",
    "comments": "14 pages; preprint of invited contribution to the Proceedings of the\n  ``50th Anniversary Summit of Artificial Intelligence'' at Monte Verita,\n  Ascona, Switzerland, 9-14 July 2006",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 31 Aug 2007 11:12:26 GMT"
      }
    ],
    "update_date": "2007-09-03",
    "authors_parsed": [
      [
        "Schmidhuber",
        "Juergen",
        ""
      ]
    ]
  },
  {
    "id": "0709.0116",
    "submitter": "Fionn Murtagh",
    "authors": "Fionn Murtagh",
    "title": "On Ultrametric Algorithmic Information",
    "comments": "Forthcoming, Computer Journal. Minor corrections 29 Oct. 2007",
    "journal-ref": "Computer Journal, 53, 405-416, 2010",
    "doi": "10.1093/comjnl/bxm084",
    "report-no": null,
    "categories": "cs.AI cs.CL",
    "license": null,
    "abstract": "  How best to quantify the information of an object, whether natural or\nartifact, is a problem of wide interest. A related problem is the computability\nof an object. We present practical examples of a new way to address this\nproblem. By giving an appropriate representation to our objects, based on a\nhierarchical coding of information, we exemplify how it is remarkably easy to\ncompute complex objects. Our algorithmic complexity is related to the length of\nthe class of objects, rather than to the length of the object.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 2 Sep 2007 17:00:40 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 29 Sep 2007 11:21:51 GMT"
      }
    ],
    "update_date": "2011-06-14",
    "authors_parsed": [
      [
        "Murtagh",
        "Fionn",
        ""
      ]
    ]
  },
  {
    "id": "0709.0178",
    "submitter": "Yasmine B. Sanderson",
    "authors": "Yasmine B. Sanderson",
    "title": "Effective Generation of Subjectively Random Binary Sequences",
    "comments": "Introduction and Section 6 revised",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.HC cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We present an algorithm for effectively generating binary sequences which\nwould be rated by people as highly likely to have been generated by a random\nprocess, such as flipping a fair coin.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 3 Sep 2007 09:32:28 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 5 Oct 2008 10:19:14 GMT"
      }
    ],
    "update_date": "2008-10-05",
    "authors_parsed": [
      [
        "Sanderson",
        "Yasmine B.",
        ""
      ]
    ]
  },
  {
    "id": "0709.0522",
    "submitter": "Florentin Smarandache",
    "authors": "Florentin Smarandache, Jean Dezert",
    "title": "Qualitative Belief Conditioning Rules (QBCR)",
    "comments": "13 pages. Presented at Fusion 2007 International Conference, Quebec\n  City, Canada, July 2007",
    "journal-ref": "Proceedings of Fusion 2007 International Conference, Quebec City,\n  Canada, July 2007",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  In this paper we extend the new family of (quantitative) Belief Conditioning\nRules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their\nqualitative counterpart for belief revision. Since the revision of quantitative\nas well as qualitative belief assignment given the occurrence of a new event\n(the conditioning constraint) can be done in many possible ways, we present\nhere only what we consider as the most appealing Qualitative Belief\nConditioning Rules (QBCR) which allow to revise the belief directly with words\nand linguistic labels and thus avoids the introduction of ad-hoc translations\nof quantitative beliefs into quantitative ones for solving the problem.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 4 Sep 2007 20:03:04 GMT"
      }
    ],
    "update_date": "2007-09-06",
    "authors_parsed": [
      [
        "Smarandache",
        "Florentin",
        ""
      ],
      [
        "Dezert",
        "Jean",
        ""
      ]
    ]
  },
  {
    "id": "0709.0674",
    "submitter": "Juergen Schmidhuber",
    "authors": "Juergen Schmidhuber",
    "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective\n  Attention, Curiosity & Creativity",
    "comments": "15 pages, 3 highly compressible low-complexity drawings. Joint\n  Invited Lecture for Algorithmic Learning Theory (ALT 2007) and Discovery\n  Science (DS 2007), Sendai, Japan, 2007",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.GR",
    "license": null,
    "abstract": "  I postulate that human or other intelligent agents function or should\nfunction as follows. They store all sensory observations as they come - the\ndata is holy. At any time, given some agent's current coding capabilities, part\nof the data is compressible by a short and hopefully fast program / description\n/ explanation / world model. In the agent's subjective eyes, such data is more\nregular and more \"beautiful\" than other data. It is well-known that knowledge\nof regularity and repeatability may improve the agent's ability to plan actions\nleading to external rewards. In absence of such rewards, however, known beauty\nis boring. Then \"interestingness\" becomes the first derivative of subjective\nbeauty: as the learning agent improves its compression algorithm, formerly\napparently random data parts become subjectively more regular and beautiful.\nSuch progress in compressibility is measured and maximized by the curiosity\ndrive: create action sequences that extend the observation history and yield\npreviously unknown / unpredictable but quickly learnable algorithmic\nregularity. We discuss how all of the above can be naturally implemented on\ncomputers, through an extension of passive unsupervised learning to the case of\nactive data selection: we reward a general reinforcement learner (with access\nto the adaptive compressor) for actions that improve the subjective\ncompressibility of the growing data. An unusually large breakthrough in\ncompressibility deserves the name \"discovery\". The \"creativity\" of artists,\ndancers, musicians, pure mathematicians can be viewed as a by-product of this\nprinciple. Several qualitative examples support this hypothesis.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 5 Sep 2007 15:20:59 GMT"
      }
    ],
    "update_date": "2007-09-06",
    "authors_parsed": [
      [
        "Schmidhuber",
        "Juergen",
        ""
      ]
    ]
  },
  {
    "id": "0709.1099",
    "submitter": "Cherif Smaili",
    "authors": "Cherif Smaili (INRIA Lorraine - LORIA), Maan El Badaoui El Najjar\n  (INRIA Lorraine - LORIA), Fran\\c{c}ois Charpillet (INRIA Lorraine - LORIA)",
    "title": "Multi-Sensor Fusion Method using Dynamic Bayesian Network for Precise\n  Vehicle Localization and Road Matching",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.RO",
    "license": null,
    "abstract": "  This paper presents a multi-sensor fusion strategy for a novel road-matching\nmethod designed to support real-time navigational features within advanced\ndriving-assistance systems. Managing multihypotheses is a useful strategy for\nthe road-matching problem. The multi-sensor fusion and multi-modal estimation\nare realized using Dynamical Bayesian Network. Experimental results, using data\nfrom Antilock Braking System (ABS) sensors, a differential Global Positioning\nSystem (GPS) receiver and an accurate digital roadmap, illustrate the\nperformances of this approach, especially in ambiguous situations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 7 Sep 2007 15:03:37 GMT"
      }
    ],
    "update_date": "2007-09-10",
    "authors_parsed": [
      [
        "Smaili",
        "Cherif",
        "",
        "INRIA Lorraine - LORIA"
      ],
      [
        "Najjar",
        "Maan El Badaoui El",
        "",
        "INRIA Lorraine - LORIA"
      ],
      [
        "Charpillet",
        "François",
        "",
        "INRIA Lorraine - LORIA"
      ]
    ]
  },
  {
    "id": "0709.1167",
    "submitter": "Marko Antonio Rodriguez",
    "authors": "Marko A. Rodriguez, Jennifer H. Watkins, Johan Bollen, Carlos\n  Gershenson",
    "title": "Using RDF to Model the Structure and Process of Systems",
    "comments": "International Conference on Complex Systems, Boston MA, October 2007",
    "journal-ref": "InterJournal of Complex Systems, 2131, ISSN: 1081-0625, February\n  2008",
    "doi": null,
    "report-no": "LAUR-07-5720",
    "categories": "cs.AI",
    "license": null,
    "abstract": "  Many systems can be described in terms of networks of discrete elements and\ntheir various relationships to one another. A semantic network, or\nmulti-relational network, is a directed labeled graph consisting of a\nheterogeneous set of entities connected by a heterogeneous set of\nrelationships. Semantic networks serve as a promising general-purpose modeling\nsubstrate for complex systems. Various standardized formats and tools are now\navailable to support practical, large-scale semantic network models. First, the\nResource Description Framework (RDF) offers a standardized semantic network\ndata model that can be further formalized by ontology modeling languages such\nas RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent\nintroduction of highly performant triple-stores (i.e. semantic network\ndatabases) allows semantic network models on the order of $10^9$ edges to be\nefficiently stored and manipulated. RDF and its related technologies are\ncurrently used extensively in the domains of computer science, digital library\nscience, and the biological sciences. This article will provide an introduction\nto RDF/RDFS/OWL and an examination of its suitability to model discrete element\ncomplex systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 8 Sep 2007 01:18:18 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 15 Oct 2007 16:00:19 GMT"
      }
    ],
    "update_date": "2008-11-03",
    "authors_parsed": [
      [
        "Rodriguez",
        "Marko A.",
        ""
      ],
      [
        "Watkins",
        "Jennifer H.",
        ""
      ],
      [
        "Bollen",
        "Johan",
        ""
      ],
      [
        "Gershenson",
        "Carlos",
        ""
      ]
    ]
  },
  {
    "id": "0709.1190",
    "submitter": "Mohsen Bayati",
    "authors": "Mohsen Bayati, Christian Borgs, Jennifer Chayes, Riccardo Zecchina",
    "title": "Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs and its\n  Relation to Linear Programs with Integer Solutions",
    "comments": "28 pages, 2 figures. Submitted to SIAM journal on Discrete\n  Mathematics on March 19, 2009; accepted for publication (in revised form)\n  August 30, 2010; published electronically July 1, 2011",
    "journal-ref": "SIAM J. Discrete Math. 2011, Vol 25, Issue 2, pp. 989-1011",
    "doi": "10.1137/090753115",
    "report-no": null,
    "categories": "cs.IT cs.AI math.IT",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We consider the general problem of finding the minimum weight $\\bm$-matching\non arbitrary graphs. We prove that, whenever the linear programming (LP)\nrelaxation of the problem has no fractional solutions, then the belief\npropagation (BP) algorithm converges to the correct solution. We also show that\nwhen the LP relaxation has a fractional solution then the BP algorithm can be\nused to solve the LP relaxation. Our proof is based on the notion of graph\ncovers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara\n2007}.\n  These results are notable in the following regards: (1) It is one of a very\nsmall number of proofs showing correctness of BP without any constraint on the\ngraph structure. (2) Variants of the proof work for both synchronous and\nasynchronous BP; it is the first proof of convergence and correctness of an\nasynchronous BP algorithm for a combinatorial optimization problem.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 8 Sep 2007 08:21:34 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 9 Feb 2008 02:56:38 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 4 Aug 2011 21:43:21 GMT"
      }
    ],
    "update_date": "2015-03-13",
    "authors_parsed": [
      [
        "Bayati",
        "Mohsen",
        ""
      ],
      [
        "Borgs",
        "Christian",
        ""
      ],
      [
        "Chayes",
        "Jennifer",
        ""
      ],
      [
        "Zecchina",
        "Riccardo",
        ""
      ]
    ]
  },
  {
    "id": "0709.1667",
    "submitter": "Federico Ricci-Tersenghi",
    "authors": "Andrea Montanari, Federico Ricci-Tersenghi and Guilhem Semerjian",
    "title": "Solving Constraint Satisfaction Problems through Belief\n  Propagation-guided decimation",
    "comments": "10 pages, 4 figures. A longer version can be found as arXiv:0904.3395\n  [cond-mat.dis-nn]",
    "journal-ref": "Proceedings of the 45th Annual Allerton Conference on\n  Communication, Control, and Computing (Monticello, IL, USA), 352-359 (2007)",
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cond-mat.dis-nn cond-mat.stat-mech cs.CC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Message passing algorithms have proved surprisingly successful in solving\nhard constraint satisfaction problems on sparse random graphs. In such\napplications, variables are fixed sequentially to satisfy the constraints.\nMessage passing is run after each step. Its outcome provides an heuristic to\nmake choices at next step. This approach has been referred to as `decimation,'\nwith reference to analogous procedures in statistical physics.\n  The behavior of decimation procedures is poorly understood. Here we consider\na simple randomized decimation algorithm based on belief propagation (BP), and\nanalyze its behavior on random k-satisfiability formulae. In particular, we\npropose a tree model for its analysis and we conjecture that it provides\nasymptotically exact predictions in the limit of large instances. This\nconjecture is confirmed by numerical simulations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 11 Sep 2007 15:48:56 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 1 Oct 2007 15:01:01 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 4 Jun 2019 11:43:45 GMT"
      }
    ],
    "update_date": "2019-06-05",
    "authors_parsed": [
      [
        "Montanari",
        "Andrea",
        ""
      ],
      [
        "Ricci-Tersenghi",
        "Federico",
        ""
      ],
      [
        "Semerjian",
        "Guilhem",
        ""
      ]
    ]
  },
  {
    "id": "0709.1699",
    "submitter": "Paul Fodor",
    "authors": "Paul Fodor",
    "title": "Efficient Tabling Mechanisms for Transaction Logic Programs",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LO cs.AI",
    "license": null,
    "abstract": "  In this paper we present efficient evaluation algorithms for the Horn\nTransaction Logic (a generalization of the regular Horn logic programs with\nstate updates). We present two complementary methods for optimizing the\nimplementation of Transaction Logic. The first method is based on tabling and\nwe modified the proof theory to table calls and answers on states (practically,\nequivalent to dynamic programming). The call-answer table is indexed on the\ncall and a signature of the state in which the call was made. The answer\ncolumns contain the answer unification and a signature of the state after the\ncall was executed. The states are signed efficiently using a technique based on\ntries and counting. The second method is based on incremental evaluation and it\napplies when the data oracle contains derived relations. The deletions and\ninsertions (executed in the transaction oracle) change the state of the\ndatabase. Using the heuristic of inertia (only a part of the state changes in\nresponse to elementary updates), most of the time it is cheaper to compute only\nthe changes in the state than to recompute the entire state from scratch. The\ntwo methods are complementary by the fact that the first method optimizes the\nevaluation when a call is repeated in the same state, and the second method\noptimizes the evaluation of a new state when a call-state pair is not found by\nthe tabling mechanism (i.e. the first method). The proof theory of Transaction\nLogic with the application of tabling and incremental evaluation is sound and\ncomplete with respect to its model theory.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 11 Sep 2007 19:00:02 GMT"
      }
    ],
    "update_date": "2007-09-12",
    "authors_parsed": [
      [
        "Fodor",
        "Paul",
        ""
      ]
    ]
  },
  {
    "id": "0709.1701",
    "submitter": "Jean Dezert",
    "authors": "Xinde Li (ICRL), Xinhan Huang (ICRL), Florentin Smarandache (UNM),\n  Jean Dezert (ONERA)",
    "title": "Enrichment of Qualitative Beliefs for Reasoning under Uncertainty",
    "comments": "12 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  This paper deals with enriched qualitative belief functions for reasoning\nunder uncertainty and for combining information expressed in natural language\nthrough linguistic labels. In this work, two possible enrichments (quantitative\nand/or qualitative) of linguistic labels are considered and operators\n(addition, multiplication, division, etc) for dealing with them are proposed\nand explained. We denote them $qe$-operators, $qe$ standing for\n\"qualitative-enriched\" operators. These operators can be seen as a direct\nextension of the classical qualitative operators ($q$-operators) proposed\nrecently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning\n(DSmT). $q$-operators are also justified in details in this paper. The\nquantitative enrichment of linguistic label is a numerical supporting degree in\n$[0,\\infty)$, while the qualitative enrichment takes its values in a finite\nordered set of linguistic values. Quantitative enrichment is less precise than\nqualitative enrichment, but it is expected more close with what human experts\ncan easily provide when expressing linguistic labels with supporting degrees.\nTwo simple examples are given to show how the fusion of qualitative-enriched\nbelief assignments can be done.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 11 Sep 2007 19:12:25 GMT"
      }
    ],
    "update_date": "2007-09-12",
    "authors_parsed": [
      [
        "Li",
        "Xinde",
        "",
        "ICRL"
      ],
      [
        "Huang",
        "Xinhan",
        "",
        "ICRL"
      ],
      [
        "Smarandache",
        "Florentin",
        "",
        "UNM"
      ],
      [
        "Dezert",
        "Jean",
        "",
        "ONERA"
      ]
    ]
  },
  {
    "id": "0709.2065",
    "submitter": "Andrei Khrennikov",
    "authors": "Andrei Khrennikov",
    "title": "Toward Psycho-robots",
    "comments": null,
    "journal-ref": "Paladyn Volume 1, Number 2, 99-108, 2010",
    "doi": "10.2478/s13230-010-0014-0",
    "report-no": null,
    "categories": "cs.AI",
    "license": null,
    "abstract": "  We try to perform geometrization of psychology by representing mental states,\n<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is\ndescribed by dynamical systems in metric mental space. We apply the mental\nspace approach for modeling of flows of unconscious and conscious information\nin the human brain. In a series of models, Models 1-4, we consider cognitive\nsystems with increasing complexity of psychological behavior determined by\nstructure of flows of ideas. Since our models are in fact models of the\nAI-type, one immediately recognizes that they can be used for creation of\nAI-systems, which we call psycho-robots, exhibiting important elements of human\npsyche. Creation of such psycho-robots may be useful improvement of domestic\nrobots. At the moment domestic robots are merely simple working devices (e.g.\nvacuum cleaners or lawn mowers) . However, in future one can expect demand in\nsystems which be able not only perform simple work tasks, but would have\nelements of human self-developing psyche. Such AI-psyche could play an\nimportant role both in relations between psycho-robots and their owners as well\nas between psycho-robots. Since the presence of a huge numbers of\npsycho-complexes is an essential characteristic of human psychology, it would\nbe interesting to model them in the AI-framework.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 13 Sep 2007 13:06:34 GMT"
      }
    ],
    "update_date": "2010-11-30",
    "authors_parsed": [
      [
        "Khrennikov",
        "Andrei",
        ""
      ]
    ]
  },
  {
    "id": "0709.2506",
    "submitter": "Tshilidzi Marwala",
    "authors": "Vukosi N. Marivate, Fulufhelo V. Nelwamodo, Tshilidzi Marwala",
    "title": "Autoencoder, Principal Component Analysis and Support Vector Regression\n  for Data Imputation",
    "comments": "9 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.DB",
    "license": null,
    "abstract": "  Data collection often results in records that have missing values or\nvariables. This investigation compares 3 different data imputation models and\nidentifies their merits by using accuracy measures. Autoencoder Neural\nNetworks, Principal components and Support Vector regression are used for\nprediction and combined with a genetic algorithm to then impute missing\nvariables. The use of PCA improves the overall performance of the autoencoder\nnetwork while the use of support vector regression shows promising potential\nfor future investigation. Accuracies of up to 97.4 % on imputation of some of\nthe variables were achieved.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 16 Sep 2007 18:15:01 GMT"
      }
    ],
    "update_date": "2007-09-18",
    "authors_parsed": [
      [
        "Marivate",
        "Vukosi N.",
        ""
      ],
      [
        "Nelwamodo",
        "Fulufhelo V.",
        ""
      ],
      [
        "Marwala",
        "Tshilidzi",
        ""
      ]
    ]
  }
]
